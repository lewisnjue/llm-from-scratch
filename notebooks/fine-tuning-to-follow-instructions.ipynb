{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Preparing a dataset for supervised instruction fine-tuning","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"# downloadin the daset \n\nimport json \nimport os \nimport urllib ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T10:50:59.219177Z","iopub.execute_input":"2025-10-23T10:50:59.219438Z","iopub.status.idle":"2025-10-23T10:50:59.226345Z","shell.execute_reply.started":"2025-10-23T10:50:59.219420Z","shell.execute_reply":"2025-10-23T10:50:59.225661Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"def download_and_load_file(file_path,url):\n    if not os.path.exists(file_path):\n        with urllib.request.urlopen(url) as response: \n            text_data = response.read().decode(\"utf-8\")\n        with open(file_path,'w',encoding='utf-8') as file:\n            file.write(text_data)\n    else:\n        with open(file_path,'r',encoding='utf-8') as file:\n            text_data = file.read()\n    with open(file_path,'r') as file:\n        data = json.load(file)\n    return data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T10:50:59.227185Z","iopub.execute_input":"2025-10-23T10:50:59.227458Z","iopub.status.idle":"2025-10-23T10:50:59.241081Z","shell.execute_reply.started":"2025-10-23T10:50:59.227440Z","shell.execute_reply":"2025-10-23T10:50:59.240400Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"file_path = \"instruction-data.json\"\nurl = (\n\"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch\"\n\"/main/ch07/01_main-chapter-code/instruction-data.json\"\n)\ndata = download_and_load_file(file_path, url)\nprint(\"Number of entries:\", len(data))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T10:50:59.242715Z","iopub.execute_input":"2025-10-23T10:50:59.242913Z","iopub.status.idle":"2025-10-23T10:50:59.416414Z","shell.execute_reply.started":"2025-10-23T10:50:59.242898Z","shell.execute_reply":"2025-10-23T10:50:59.415615Z"}},"outputs":[{"name":"stdout","text":"Number of entries: 1100\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"print(\"Example entry:\\n\", data[50])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T10:50:59.417214Z","iopub.execute_input":"2025-10-23T10:50:59.417573Z","iopub.status.idle":"2025-10-23T10:50:59.422048Z","shell.execute_reply.started":"2025-10-23T10:50:59.417545Z","shell.execute_reply":"2025-10-23T10:50:59.421217Z"}},"outputs":[{"name":"stdout","text":"Example entry:\n {'instruction': 'Identify the correct spelling of the following word.', 'input': 'Ocassion', 'output': \"The correct spelling is 'Occasion.'\"}\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"\"\"\"\nAlpaca prompt style since it is one of the most\npopular ones, largely because it helped define the original approach to fine-tuning.\n\"\"\"\ndef format_input(entry):\n    instruction_text = (\n    f\"Below is an instruction that describes a task. \"\n    f\"Write a response that appropriately completes the request.\"\n    f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n    )\n    input_text = (\n    f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n    )\n    return instruction_text + input_text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T10:50:59.422923Z","iopub.execute_input":"2025-10-23T10:50:59.423160Z","iopub.status.idle":"2025-10-23T10:50:59.436510Z","shell.execute_reply.started":"2025-10-23T10:50:59.423138Z","shell.execute_reply":"2025-10-23T10:50:59.435706Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"model_input = format_input(data[50])\ndesired_response = f\"\\n\\n### Response:\\n{data[50]['output']}\"\nprint(model_input + desired_response)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T10:50:59.437450Z","iopub.execute_input":"2025-10-23T10:50:59.437686Z","iopub.status.idle":"2025-10-23T10:50:59.451065Z","shell.execute_reply.started":"2025-10-23T10:50:59.437665Z","shell.execute_reply":"2025-10-23T10:50:59.450393Z"}},"outputs":[{"name":"stdout","text":"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nIdentify the correct spelling of the following word.\n\n### Input:\nOcassion\n\n### Response:\nThe correct spelling is 'Occasion.'\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"model_input = format_input(data[999])\ndesired_response = f\"\\n\\n### Response:\\n{data[999]['output']}\"\nprint(model_input + desired_response)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T10:50:59.451919Z","iopub.execute_input":"2025-10-23T10:50:59.452127Z","iopub.status.idle":"2025-10-23T10:50:59.465163Z","shell.execute_reply.started":"2025-10-23T10:50:59.452108Z","shell.execute_reply":"2025-10-23T10:50:59.464412Z"}},"outputs":[{"name":"stdout","text":"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nWhat is an antonym of 'complicated'?\n\n### Response:\nAn antonym of 'complicated' is 'simple'.\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"train_portion = int(len(data) * 0.85)\ntest_portion = int(len(data) * 0.1) \nval_portion = len(data) - train_portion - test_portion \ntrain_data = data[:train_portion] \ntest_data = data[train_portion:train_portion + test_portion] \nval_data = data[train_portion + test_portion:] \nprint(\"Training set length:\", len(train_data))\nprint(\"Validation set length:\", len(val_data))\nprint(\"Test set length:\", len(test_data))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T10:50:59.465979Z","iopub.execute_input":"2025-10-23T10:50:59.466347Z","iopub.status.idle":"2025-10-23T10:50:59.480479Z","shell.execute_reply.started":"2025-10-23T10:50:59.466322Z","shell.execute_reply":"2025-10-23T10:50:59.479688Z"}},"outputs":[{"name":"stdout","text":"Training set length: 935\nValidation set length: 55\nTest set length: 110\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import torch \nfrom torch.utils.data import Dataset, DataLoader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T10:50:59.482477Z","iopub.execute_input":"2025-10-23T10:50:59.482666Z","iopub.status.idle":"2025-10-23T10:51:03.355019Z","shell.execute_reply.started":"2025-10-23T10:50:59.482644Z","shell.execute_reply":"2025-10-23T10:51:03.354288Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"class InstructionDataset(Dataset):\n    def __init__(self,data,tokenizer): \n        self.data = data\n        self.encoded_texts = []\n        for entry in data:\n            instruction_plus_input = format_input(entry)\n            response_text = f\"\\n\\n### Response:\\n{entry['output']}\"\n            full_text = instruction_plus_input + response_text\n            self.encoded_texts.append(\n                tokenizer.encode(full_text)\n            )\n    def __getitem__(self,index):\n        return self.encoded_texts[index]\n    def __len__(self):\n        return len(self.data)\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T10:51:03.355855Z","iopub.execute_input":"2025-10-23T10:51:03.356268Z","iopub.status.idle":"2025-10-23T10:51:03.361948Z","shell.execute_reply.started":"2025-10-23T10:51:03.356227Z","shell.execute_reply":"2025-10-23T10:51:03.361115Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"import tiktoken\ntokenizer = tiktoken.get_encoding(\"gpt2\")\nprint(tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T10:51:03.362633Z","iopub.execute_input":"2025-10-23T10:51:03.362868Z","iopub.status.idle":"2025-10-23T10:51:04.999777Z","shell.execute_reply.started":"2025-10-23T10:51:03.362846Z","shell.execute_reply":"2025-10-23T10:51:04.999143Z"}},"outputs":[{"name":"stdout","text":"[50256]\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"def custom_collate_draft_1(\n    batch,\n    pad_token_id=50256,\n    device=\"cpu\"\n):\n    batch_max_lenght = max(len(item)+1 for item in batch)\n    inputs_lst = [] \n    for item in batch: \n        new_item = item.copy() \n        new_item += [pad_token_id] \n        padded = (\n            new_item + [ pad_token_id] * (batch_max_lenght - len(new_item))\n        )\n        inputs = torch.tensor(padded[:-1]) \n        inputs_lst.append(inputs)\n    inputs_tensor = torch.stack(inputs_lst).to(device)\n    return inputs_tensor \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T10:51:05.000807Z","iopub.execute_input":"2025-10-23T10:51:05.001077Z","iopub.status.idle":"2025-10-23T10:51:05.007297Z","shell.execute_reply.started":"2025-10-23T10:51:05.001052Z","shell.execute_reply":"2025-10-23T10:51:05.006401Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"inputs_1 = [0, 1, 2, 3, 4]\ninputs_2 = [5, 6]\ninputs_3 = [7, 8, 9]\nbatch = (\ninputs_1,\ninputs_2,\ninputs_3\n)\nprint(custom_collate_draft_1(batch))\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T10:51:43.429362Z","iopub.execute_input":"2025-10-23T10:51:43.429922Z","iopub.status.idle":"2025-10-23T10:51:43.469578Z","shell.execute_reply.started":"2025-10-23T10:51:43.429897Z","shell.execute_reply":"2025-10-23T10:51:43.468963Z"}},"outputs":[{"name":"stdout","text":"tensor([[    0,     1,     2,     3,     4],\n        [    5,     6, 50256, 50256, 50256],\n        [    7,     8,     9, 50256, 50256]])\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"def custom_collate_draft_2(\n    batch,\n    pad_token_id=50256,\n    device=\"cpu\"\n):\n    batch_max_length = max(len(item)+1 for item in batch)\n    inputs_lst , targets_lst = [],[]\n    for item in batch:\n        new_item = item.copy() # its a tensor :)\n        new_item += [pad_token_id]\n        padded =(\n            new_item +[ pad_token_id] * (batch_max_length - len(new_item))\n            \n        )\n        inputs = torch.tensor(padded[:-1])\n        targets = torch.tensor(padded[1:])\n        inputs_lst.append(inputs)\n        targets_lst.append(targets)\n    inputs_tensor = torch.stack(inputs_lst).to(device)\n    targets_tensor = torch.stack(targets_lst).to(device)\n    return inputs_tensor,targets_tensor","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T10:51:46.119624Z","iopub.execute_input":"2025-10-23T10:51:46.120105Z","iopub.status.idle":"2025-10-23T10:51:46.125284Z","shell.execute_reply.started":"2025-10-23T10:51:46.120082Z","shell.execute_reply":"2025-10-23T10:51:46.124330Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"inputs, targets = custom_collate_draft_2(batch)\nprint(inputs)\nprint(targets)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T10:51:46.640762Z","iopub.execute_input":"2025-10-23T10:51:46.641015Z","iopub.status.idle":"2025-10-23T10:51:46.647021Z","shell.execute_reply.started":"2025-10-23T10:51:46.640998Z","shell.execute_reply":"2025-10-23T10:51:46.646228Z"}},"outputs":[{"name":"stdout","text":"tensor([[    0,     1,     2,     3,     4],\n        [    5,     6, 50256, 50256, 50256],\n        [    7,     8,     9, 50256, 50256]])\ntensor([[    1,     2,     3,     4, 50256],\n        [    6, 50256, 50256, 50256, 50256],\n        [    8,     9, 50256, 50256, 50256]])\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"def custom_collate_fn(\n    batch,\n    pad_token_id=50256,\n    ignore_index = -100,\n    allowed_max_length = None,\n    device=\"cpu\"\n):\n    batch_max_length = max(len(item)+1 for item in batch)\n    inputs_lst , targets_lst = [],[]\n    for item in batch:\n        new_item = item.copy() # its a tensor :)\n        new_item += [pad_token_id]\n        padded =(\n            new_item +[ pad_token_id] * (batch_max_length - len(new_item))\n            \n        )\n        inputs = torch.tensor(padded[:-1])\n        targets = torch.tensor(padded[1:])\n         #--------\n        mask = targets == pad_token_id \n        indices = torch.nonzero(mask).squeeze() \n        if indices.numel() > 1:\n            targets[indices[1:]] = ignore_index\n        if allowed_max_length is not None:\n            inputs = inputs[:allowed_max_length]\n            targets = targets[:allowed_max_length]\n        #------\n        inputs_lst.append(inputs)\n        targets_lst.append(targets)\n    inputs_tensor = torch.stack(inputs_lst).to(device)\n    targets_tensor = torch.stack(targets_lst).to(device)\n    return inputs_tensor,targets_tensor","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T10:51:48.492710Z","iopub.execute_input":"2025-10-23T10:51:48.493176Z","iopub.status.idle":"2025-10-23T10:51:48.499029Z","shell.execute_reply.started":"2025-10-23T10:51:48.493155Z","shell.execute_reply":"2025-10-23T10:51:48.498189Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"inputs, targets = custom_collate_fn(batch)\nprint(inputs)\nprint(targets)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T10:51:50.600758Z","iopub.execute_input":"2025-10-23T10:51:50.601010Z","iopub.status.idle":"2025-10-23T10:51:50.619892Z","shell.execute_reply.started":"2025-10-23T10:51:50.600992Z","shell.execute_reply":"2025-10-23T10:51:50.619309Z"}},"outputs":[{"name":"stdout","text":"tensor([[    0,     1,     2,     3,     4],\n        [    5,     6, 50256, 50256, 50256],\n        [    7,     8,     9, 50256, 50256]])\ntensor([[    1,     2,     3,     4, 50256],\n        [    6, 50256,  -100,  -100,  -100],\n        [    8,     9, 50256,  -100,  -100]])\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T10:51:50.947941Z","iopub.execute_input":"2025-10-23T10:51:50.948621Z","iopub.status.idle":"2025-10-23T10:51:51.010127Z","shell.execute_reply.started":"2025-10-23T10:51:50.948599Z","shell.execute_reply":"2025-10-23T10:51:51.009423Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"from functools import partial\ncustomized_collate_fn = partial(\ncustom_collate_fn,\ndevice=device,\nallowed_max_length=1024\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T10:51:52.650623Z","iopub.execute_input":"2025-10-23T10:51:52.651096Z","iopub.status.idle":"2025-10-23T10:51:52.654472Z","shell.execute_reply.started":"2025-10-23T10:51:52.651070Z","shell.execute_reply":"2025-10-23T10:51:52.653730Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"\nnum_workers = 0 \nbatch_size = 4\ntorch.manual_seed(123)\n\ntrain_dataset = InstructionDataset(train_data, tokenizer)\ntrain_loader = DataLoader(\ntrain_dataset,\nbatch_size=batch_size,\ncollate_fn=customized_collate_fn,\nshuffle=True,\ndrop_last=True,\nnum_workers=num_workers\n)\nval_dataset = InstructionDataset(val_data, tokenizer)\nval_loader = DataLoader(\nval_dataset,\nbatch_size=batch_size,\ncollate_fn=customized_collate_fn,\nshuffle=False,\ndrop_last=False,\nnum_workers=num_workers\n)\ntest_dataset = InstructionDataset(test_data, tokenizer)\ntest_loader = DataLoader(\ntest_dataset,\nbatch_size=batch_size,\ncollate_fn=customized_collate_fn,\nshuffle=False,\ndrop_last=False,\nnum_workers=num_workers)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T10:59:51.720154Z","iopub.execute_input":"2025-10-23T10:59:51.720522Z","iopub.status.idle":"2025-10-23T10:59:51.780544Z","shell.execute_reply.started":"2025-10-23T10:59:51.720498Z","shell.execute_reply":"2025-10-23T10:59:51.779705Z"}},"outputs":[],"execution_count":55},{"cell_type":"code","source":"print(\"Train loader:\")\nfor inputs, targets in train_loader:\n    print(inputs.shape, targets.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T10:59:53.614288Z","iopub.execute_input":"2025-10-23T10:59:53.614944Z","iopub.status.idle":"2025-10-23T10:59:53.722398Z","shell.execute_reply.started":"2025-10-23T10:59:53.614920Z","shell.execute_reply":"2025-10-23T10:59:53.721639Z"}},"outputs":[{"name":"stdout","text":"Train loader:\ntorch.Size([4, 61]) torch.Size([4, 61])\ntorch.Size([4, 58]) torch.Size([4, 58])\ntorch.Size([4, 62]) torch.Size([4, 62])\ntorch.Size([4, 76]) torch.Size([4, 76])\ntorch.Size([4, 73]) torch.Size([4, 73])\ntorch.Size([4, 55]) torch.Size([4, 55])\ntorch.Size([4, 68]) torch.Size([4, 68])\ntorch.Size([4, 68]) torch.Size([4, 68])\ntorch.Size([4, 65]) torch.Size([4, 65])\ntorch.Size([4, 57]) torch.Size([4, 57])\ntorch.Size([4, 72]) torch.Size([4, 72])\ntorch.Size([4, 60]) torch.Size([4, 60])\ntorch.Size([4, 80]) torch.Size([4, 80])\ntorch.Size([4, 64]) torch.Size([4, 64])\ntorch.Size([4, 63]) torch.Size([4, 63])\ntorch.Size([4, 67]) torch.Size([4, 67])\ntorch.Size([4, 61]) torch.Size([4, 61])\ntorch.Size([4, 62]) torch.Size([4, 62])\ntorch.Size([4, 68]) torch.Size([4, 68])\ntorch.Size([4, 75]) torch.Size([4, 75])\ntorch.Size([4, 52]) torch.Size([4, 52])\ntorch.Size([4, 62]) torch.Size([4, 62])\ntorch.Size([4, 67]) torch.Size([4, 67])\ntorch.Size([4, 68]) torch.Size([4, 68])\ntorch.Size([4, 65]) torch.Size([4, 65])\ntorch.Size([4, 67]) torch.Size([4, 67])\ntorch.Size([4, 60]) torch.Size([4, 60])\ntorch.Size([4, 77]) torch.Size([4, 77])\ntorch.Size([4, 59]) torch.Size([4, 59])\ntorch.Size([4, 69]) torch.Size([4, 69])\ntorch.Size([4, 65]) torch.Size([4, 65])\ntorch.Size([4, 79]) torch.Size([4, 79])\ntorch.Size([4, 71]) torch.Size([4, 71])\ntorch.Size([4, 52]) torch.Size([4, 52])\ntorch.Size([4, 66]) torch.Size([4, 66])\ntorch.Size([4, 60]) torch.Size([4, 60])\ntorch.Size([4, 83]) torch.Size([4, 83])\ntorch.Size([4, 61]) torch.Size([4, 61])\ntorch.Size([4, 68]) torch.Size([4, 68])\ntorch.Size([4, 59]) torch.Size([4, 59])\ntorch.Size([4, 58]) torch.Size([4, 58])\ntorch.Size([4, 80]) torch.Size([4, 80])\ntorch.Size([4, 71]) torch.Size([4, 71])\ntorch.Size([4, 68]) torch.Size([4, 68])\ntorch.Size([4, 63]) torch.Size([4, 63])\ntorch.Size([4, 69]) torch.Size([4, 69])\ntorch.Size([4, 48]) torch.Size([4, 48])\ntorch.Size([4, 65]) torch.Size([4, 65])\ntorch.Size([4, 68]) torch.Size([4, 68])\ntorch.Size([4, 61]) torch.Size([4, 61])\ntorch.Size([4, 60]) torch.Size([4, 60])\ntorch.Size([4, 57]) torch.Size([4, 57])\ntorch.Size([4, 59]) torch.Size([4, 59])\ntorch.Size([4, 57]) torch.Size([4, 57])\ntorch.Size([4, 63]) torch.Size([4, 63])\ntorch.Size([4, 69]) torch.Size([4, 69])\ntorch.Size([4, 60]) torch.Size([4, 60])\ntorch.Size([4, 63]) torch.Size([4, 63])\ntorch.Size([4, 65]) torch.Size([4, 65])\ntorch.Size([4, 65]) torch.Size([4, 65])\ntorch.Size([4, 60]) torch.Size([4, 60])\ntorch.Size([4, 76]) torch.Size([4, 76])\ntorch.Size([4, 62]) torch.Size([4, 62])\ntorch.Size([4, 66]) torch.Size([4, 66])\ntorch.Size([4, 71]) torch.Size([4, 71])\ntorch.Size([4, 71]) torch.Size([4, 71])\ntorch.Size([4, 69]) torch.Size([4, 69])\ntorch.Size([4, 91]) torch.Size([4, 91])\ntorch.Size([4, 62]) torch.Size([4, 62])\ntorch.Size([4, 65]) torch.Size([4, 65])\ntorch.Size([4, 64]) torch.Size([4, 64])\ntorch.Size([4, 58]) torch.Size([4, 58])\ntorch.Size([4, 64]) torch.Size([4, 64])\ntorch.Size([4, 67]) torch.Size([4, 67])\ntorch.Size([4, 66]) torch.Size([4, 66])\ntorch.Size([4, 61]) torch.Size([4, 61])\ntorch.Size([4, 54]) torch.Size([4, 54])\ntorch.Size([4, 64]) torch.Size([4, 64])\ntorch.Size([4, 52]) torch.Size([4, 52])\ntorch.Size([4, 65]) torch.Size([4, 65])\ntorch.Size([4, 59]) torch.Size([4, 59])\ntorch.Size([4, 75]) torch.Size([4, 75])\ntorch.Size([4, 56]) torch.Size([4, 56])\ntorch.Size([4, 89]) torch.Size([4, 89])\ntorch.Size([4, 59]) torch.Size([4, 59])\ntorch.Size([4, 52]) torch.Size([4, 52])\ntorch.Size([4, 60]) torch.Size([4, 60])\ntorch.Size([4, 88]) torch.Size([4, 88])\ntorch.Size([4, 56]) torch.Size([4, 56])\ntorch.Size([4, 83]) torch.Size([4, 83])\ntorch.Size([4, 58]) torch.Size([4, 58])\ntorch.Size([4, 83]) torch.Size([4, 83])\ntorch.Size([4, 70]) torch.Size([4, 70])\ntorch.Size([4, 66]) torch.Size([4, 66])\ntorch.Size([4, 65]) torch.Size([4, 65])\ntorch.Size([4, 59]) torch.Size([4, 59])\ntorch.Size([4, 59]) torch.Size([4, 59])\ntorch.Size([4, 74]) torch.Size([4, 74])\ntorch.Size([4, 76]) torch.Size([4, 76])\ntorch.Size([4, 75]) torch.Size([4, 75])\ntorch.Size([4, 67]) torch.Size([4, 67])\ntorch.Size([4, 55]) torch.Size([4, 55])\ntorch.Size([4, 60]) torch.Size([4, 60])\ntorch.Size([4, 75]) torch.Size([4, 75])\ntorch.Size([4, 83]) torch.Size([4, 83])\ntorch.Size([4, 74]) torch.Size([4, 74])\ntorch.Size([4, 69]) torch.Size([4, 69])\ntorch.Size([4, 56]) torch.Size([4, 56])\ntorch.Size([4, 67]) torch.Size([4, 67])\ntorch.Size([4, 65]) torch.Size([4, 65])\ntorch.Size([4, 60]) torch.Size([4, 60])\ntorch.Size([4, 59]) torch.Size([4, 59])\ntorch.Size([4, 60]) torch.Size([4, 60])\ntorch.Size([4, 58]) torch.Size([4, 58])\ntorch.Size([4, 63]) torch.Size([4, 63])\ntorch.Size([4, 66]) torch.Size([4, 66])\ntorch.Size([4, 69]) torch.Size([4, 69])\ntorch.Size([4, 80]) torch.Size([4, 80])\ntorch.Size([4, 71]) torch.Size([4, 71])\ntorch.Size([4, 60]) torch.Size([4, 60])\ntorch.Size([4, 61]) torch.Size([4, 61])\ntorch.Size([4, 61]) torch.Size([4, 61])\ntorch.Size([4, 58]) torch.Size([4, 58])\ntorch.Size([4, 58]) torch.Size([4, 58])\ntorch.Size([4, 69]) torch.Size([4, 69])\ntorch.Size([4, 71]) torch.Size([4, 71])\ntorch.Size([4, 67]) torch.Size([4, 67])\ntorch.Size([4, 61]) torch.Size([4, 61])\ntorch.Size([4, 68]) torch.Size([4, 68])\ntorch.Size([4, 66]) torch.Size([4, 66])\ntorch.Size([4, 63]) torch.Size([4, 63])\ntorch.Size([4, 55]) torch.Size([4, 55])\ntorch.Size([4, 70]) torch.Size([4, 70])\ntorch.Size([4, 87]) torch.Size([4, 87])\ntorch.Size([4, 66]) torch.Size([4, 66])\ntorch.Size([4, 68]) torch.Size([4, 68])\ntorch.Size([4, 64]) torch.Size([4, 64])\ntorch.Size([4, 63]) torch.Size([4, 63])\ntorch.Size([4, 68]) torch.Size([4, 68])\ntorch.Size([4, 63]) torch.Size([4, 63])\ntorch.Size([4, 71]) torch.Size([4, 71])\ntorch.Size([4, 62]) torch.Size([4, 62])\ntorch.Size([4, 63]) torch.Size([4, 63])\ntorch.Size([4, 68]) torch.Size([4, 68])\ntorch.Size([4, 71]) torch.Size([4, 71])\ntorch.Size([4, 54]) torch.Size([4, 54])\ntorch.Size([4, 61]) torch.Size([4, 61])\ntorch.Size([4, 56]) torch.Size([4, 56])\ntorch.Size([4, 62]) torch.Size([4, 62])\ntorch.Size([4, 65]) torch.Size([4, 65])\ntorch.Size([4, 67]) torch.Size([4, 67])\ntorch.Size([4, 63]) torch.Size([4, 63])\ntorch.Size([4, 65]) torch.Size([4, 65])\ntorch.Size([4, 61]) torch.Size([4, 61])\ntorch.Size([4, 60]) torch.Size([4, 60])\ntorch.Size([4, 64]) torch.Size([4, 64])\ntorch.Size([4, 60]) torch.Size([4, 60])\ntorch.Size([4, 58]) torch.Size([4, 58])\ntorch.Size([4, 72]) torch.Size([4, 72])\ntorch.Size([4, 69]) torch.Size([4, 69])\ntorch.Size([4, 58]) torch.Size([4, 58])\ntorch.Size([4, 64]) torch.Size([4, 64])\ntorch.Size([4, 70]) torch.Size([4, 70])\ntorch.Size([4, 66]) torch.Size([4, 66])\ntorch.Size([4, 53]) torch.Size([4, 53])\ntorch.Size([4, 57]) torch.Size([4, 57])\ntorch.Size([4, 61]) torch.Size([4, 61])\ntorch.Size([4, 72]) torch.Size([4, 72])\ntorch.Size([4, 63]) torch.Size([4, 63])\ntorch.Size([4, 64]) torch.Size([4, 64])\ntorch.Size([4, 68]) torch.Size([4, 68])\ntorch.Size([4, 65]) torch.Size([4, 65])\ntorch.Size([4, 61]) torch.Size([4, 61])\ntorch.Size([4, 62]) torch.Size([4, 62])\ntorch.Size([4, 74]) torch.Size([4, 74])\ntorch.Size([4, 73]) torch.Size([4, 73])\ntorch.Size([4, 62]) torch.Size([4, 62])\ntorch.Size([4, 80]) torch.Size([4, 80])\ntorch.Size([4, 68]) torch.Size([4, 68])\ntorch.Size([4, 60]) torch.Size([4, 60])\ntorch.Size([4, 70]) torch.Size([4, 70])\ntorch.Size([4, 58]) torch.Size([4, 58])\ntorch.Size([4, 67]) torch.Size([4, 67])\ntorch.Size([4, 91]) torch.Size([4, 91])\ntorch.Size([4, 56]) torch.Size([4, 56])\ntorch.Size([4, 61]) torch.Size([4, 61])\ntorch.Size([4, 58]) torch.Size([4, 58])\ntorch.Size([4, 66]) torch.Size([4, 66])\ntorch.Size([4, 71]) torch.Size([4, 71])\ntorch.Size([4, 80]) torch.Size([4, 80])\ntorch.Size([4, 61]) torch.Size([4, 61])\ntorch.Size([4, 81]) torch.Size([4, 81])\ntorch.Size([4, 56]) torch.Size([4, 56])\ntorch.Size([4, 74]) torch.Size([4, 74])\ntorch.Size([4, 61]) torch.Size([4, 61])\ntorch.Size([4, 82]) torch.Size([4, 82])\ntorch.Size([4, 63]) torch.Size([4, 63])\ntorch.Size([4, 61]) torch.Size([4, 61])\ntorch.Size([4, 83]) torch.Size([4, 83])\ntorch.Size([4, 62]) torch.Size([4, 62])\ntorch.Size([4, 68]) torch.Size([4, 68])\ntorch.Size([4, 63]) torch.Size([4, 63])\ntorch.Size([4, 67]) torch.Size([4, 67])\ntorch.Size([4, 64]) torch.Size([4, 64])\ntorch.Size([4, 61]) torch.Size([4, 61])\ntorch.Size([4, 77]) torch.Size([4, 77])\ntorch.Size([4, 91]) torch.Size([4, 91])\ntorch.Size([4, 69]) torch.Size([4, 69])\ntorch.Size([4, 55]) torch.Size([4, 55])\ntorch.Size([4, 64]) torch.Size([4, 64])\ntorch.Size([4, 61]) torch.Size([4, 61])\ntorch.Size([4, 60]) torch.Size([4, 60])\ntorch.Size([4, 62]) torch.Size([4, 62])\ntorch.Size([4, 75]) torch.Size([4, 75])\ntorch.Size([4, 58]) torch.Size([4, 58])\ntorch.Size([4, 64]) torch.Size([4, 64])\ntorch.Size([4, 66]) torch.Size([4, 66])\ntorch.Size([4, 66]) torch.Size([4, 66])\ntorch.Size([4, 78]) torch.Size([4, 78])\ntorch.Size([4, 76]) torch.Size([4, 76])\ntorch.Size([4, 66]) torch.Size([4, 66])\ntorch.Size([4, 65]) torch.Size([4, 65])\ntorch.Size([4, 64]) torch.Size([4, 64])\ntorch.Size([4, 59]) torch.Size([4, 59])\ntorch.Size([4, 83]) torch.Size([4, 83])\ntorch.Size([4, 59]) torch.Size([4, 59])\ntorch.Size([4, 66]) torch.Size([4, 66])\ntorch.Size([4, 64]) torch.Size([4, 64])\ntorch.Size([4, 73]) torch.Size([4, 73])\ntorch.Size([4, 74]) torch.Size([4, 74])\ntorch.Size([4, 60]) torch.Size([4, 60])\ntorch.Size([4, 69]) torch.Size([4, 69])\ntorch.Size([4, 64]) torch.Size([4, 64])\n","output_type":"stream"}],"execution_count":56},{"cell_type":"code","source":"# LOADING A PRE-TRAINED LLM \n# Copyright (c) Sebastian Raschka under Apache License 2.0 (see LICENSE.txt).\n# Source for \"Build a Large Language Model From Scratch\"\n#   - https://www.manning.com/books/build-a-large-language-model-from-scratch\n# Code: https://github.com/rasbt/LLMs-from-scratch\n\n\nimport os\n\nimport requests\nimport json\nimport numpy as np\nimport tensorflow as tf\nfrom tqdm import tqdm\n\n\ndef download_and_load_gpt2(model_size, models_dir):\n    # Validate model size\n    allowed_sizes = (\"124M\", \"355M\", \"774M\", \"1558M\")\n    if model_size not in allowed_sizes:\n        raise ValueError(f\"Model size not in {allowed_sizes}\")\n\n    # Define paths\n    model_dir = os.path.join(models_dir, model_size)\n    base_url = \"https://openaipublic.blob.core.windows.net/gpt-2/models\"\n    backup_base_url = \"https://f001.backblazeb2.com/file/LLMs-from-scratch/gpt2\"\n    filenames = [\n        \"checkpoint\", \"encoder.json\", \"hparams.json\",\n        \"model.ckpt.data-00000-of-00001\", \"model.ckpt.index\",\n        \"model.ckpt.meta\", \"vocab.bpe\"\n    ]\n\n    # Download files\n    os.makedirs(model_dir, exist_ok=True)\n    for filename in filenames:\n        file_url = os.path.join(base_url, model_size, filename)\n        backup_url = os.path.join(backup_base_url, model_size, filename)\n        file_path = os.path.join(model_dir, filename)\n        download_file(file_url, file_path, backup_url)\n\n    # Load settings and params\n    tf_ckpt_path = tf.train.latest_checkpoint(model_dir)\n    settings = json.load(open(os.path.join(model_dir, \"hparams.json\"), \"r\", encoding=\"utf-8\"))\n    params = load_gpt2_params_from_tf_ckpt(tf_ckpt_path, settings)\n\n    return settings, params\n\n\ndef download_file(url, destination, backup_url=None):\n    def _attempt_download(download_url):\n        response = requests.get(download_url, stream=True, timeout=60)\n        response.raise_for_status()\n\n        file_size = int(response.headers.get(\"Content-Length\", 0))\n\n        # Check if file exists and has same size\n        if os.path.exists(destination):\n            file_size_local = os.path.getsize(destination)\n            if file_size and file_size == file_size_local:\n                print(f\"File already exists and is up-to-date: {destination}\")\n                return True\n\n        block_size = 1024  # 1 KB\n        desc = os.path.basename(download_url)\n        with tqdm(total=file_size, unit=\"iB\", unit_scale=True, desc=desc) as progress_bar:\n            with open(destination, \"wb\") as file:\n                for chunk in response.iter_content(chunk_size=block_size):\n                    if chunk:\n                        file.write(chunk)\n                        progress_bar.update(len(chunk))\n        return True\n\n    try:\n        if _attempt_download(url):\n            return\n    except requests.exceptions.RequestException:\n        if backup_url is not None:\n            print(f\"Primary URL ({url}) failed. Attempting backup URL: {backup_url}\")\n            try:\n                if _attempt_download(backup_url):\n                    return\n            except requests.exceptions.RequestException:\n                pass\n\n        error_message = (\n            f\"Failed to download from both primary URL ({url})\"\n            f\"{' and backup URL (' + backup_url + ')' if backup_url else ''}.\"\n            \"\\nCheck your internet connection or the file availability.\\n\"\n            \"For help, visit: https://github.com/rasbt/LLMs-from-scratch/discussions/273\"\n        )\n        print(error_message)\n    except Exception as e:\n        print(f\"An unexpected error occurred: {e}\")\n\n\n# Alternative way using `requests`\n\"\"\"\ndef download_file(url, destination):\n    # Send a GET request to download the file in streaming mode\n    response = requests.get(url, stream=True)\n\n    # Get the total file size from headers, defaulting to 0 if not present\n    file_size = int(response.headers.get(\"content-length\", 0))\n\n    # Check if file exists and has the same size\n    if os.path.exists(destination):\n        file_size_local = os.path.getsize(destination)\n        if file_size == file_size_local:\n            print(f\"File already exists and is up-to-date: {destination}\")\n            return\n\n    # Define the block size for reading the file\n    block_size = 1024  # 1 Kilobyte\n\n    # Initialize the progress bar with total file size\n    progress_bar_description = url.split(\"/\")[-1]  # Extract filename from URL\n    with tqdm(total=file_size, unit=\"iB\", unit_scale=True, desc=progress_bar_description) as progress_bar:\n        # Open the destination file in binary write mode\n        with open(destination, \"wb\") as file:\n            # Iterate over the file data in chunks\n            for chunk in response.iter_content(block_size):\n                progress_bar.update(len(chunk))  # Update progress bar\n                file.write(chunk)  # Write the chunk to the file\n\"\"\"\n\n\ndef load_gpt2_params_from_tf_ckpt(ckpt_path, settings):\n    # Initialize parameters dictionary with empty blocks for each layer\n    params = {\"blocks\": [{} for _ in range(settings[\"n_layer\"])]}\n\n    # Iterate over each variable in the checkpoint\n    for name, _ in tf.train.list_variables(ckpt_path):\n        # Load the variable and remove singleton dimensions\n        variable_array = np.squeeze(tf.train.load_variable(ckpt_path, name))\n\n        # Process the variable name to extract relevant parts\n        variable_name_parts = name.split(\"/\")[1:]  # Skip the 'model/' prefix\n\n        # Identify the target dictionary for the variable\n        target_dict = params\n        if variable_name_parts[0].startswith(\"h\"):\n            layer_number = int(variable_name_parts[0][1:])\n            target_dict = params[\"blocks\"][layer_number]\n\n        # Recursively access or create nested dictionaries\n        for key in variable_name_parts[1:-1]:\n            target_dict = target_dict.setdefault(key, {})\n\n        # Assign the variable array to the last key\n        last_key = variable_name_parts[-1]\n        target_dict[last_key] = variable_array\n\n    return params\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T11:00:06.483468Z","iopub.execute_input":"2025-10-23T11:00:06.484153Z","iopub.status.idle":"2025-10-23T11:00:06.496971Z","shell.execute_reply.started":"2025-10-23T11:00:06.484130Z","shell.execute_reply":"2025-10-23T11:00:06.496148Z"}},"outputs":[],"execution_count":60},{"cell_type":"code","source":"import torch \nimport torch.nn as nn \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T11:00:10.531260Z","iopub.execute_input":"2025-10-23T11:00:10.531740Z","iopub.status.idle":"2025-10-23T11:00:10.535022Z","shell.execute_reply.started":"2025-10-23T11:00:10.531720Z","shell.execute_reply":"2025-10-23T11:00:10.534372Z"}},"outputs":[],"execution_count":61},{"cell_type":"code","source":"\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n        super().__init__()\n        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n\n        self.d_out = d_out\n        self.num_heads = num_heads\n        self.head_dim = d_out // num_heads\n\n        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n\n        self.out_proj = nn.Linear(d_out, d_out)\n        self.dropout = nn.Dropout(dropout)\n\n        mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n        self.register_buffer(\"mask\", mask)\n\n    def forward(self, x):\n        B, T, D_in = x.shape\n\n        Q = self.W_query(x)  # (B, T, D_out)\n        K = self.W_key(x)    # (B, T, D_out)\n        V = self.W_value(x)  # (B, T, D_out)\n\n\n        Q = Q.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)  # (B, num_heads, T, head_dim)\n        K = K.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n        V = V.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n\n        # Attention scores\n        attn_scores = (Q @ K.transpose(-2, -1)) / (self.head_dim ** 0.5)  # (B, num_heads, T, T)\n\n        mask = self.mask[:T, :T].bool()\n        attn_scores = attn_scores.masked_fill(mask[None, None, :, :], float('-inf'))\n\n        # Softmax and dropout\n        attn_weights = torch.softmax(attn_scores, dim=-1)\n        attn_weights = self.dropout(attn_weights)\n\n        # Apply attention\n        context = attn_weights @ V  # (B, num_heads, T, head_dim)\n\n        # Merge heads back\n        context = context.transpose(1, 2).contiguous().view(B, T, self.d_out)\n        context = self.out_proj(context)\n\n        return context\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T11:00:11.781961Z","iopub.execute_input":"2025-10-23T11:00:11.782361Z","iopub.status.idle":"2025-10-23T11:00:11.794666Z","shell.execute_reply.started":"2025-10-23T11:00:11.782328Z","shell.execute_reply":"2025-10-23T11:00:11.793912Z"}},"outputs":[],"execution_count":62},{"cell_type":"code","source":"class GELU(nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        return 0.5 * x * (1 + torch.tanh(\n        torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n        (x + 0.044715 * torch.pow(x, 3))\n        ))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T11:00:13.644107Z","iopub.execute_input":"2025-10-23T11:00:13.644390Z","iopub.status.idle":"2025-10-23T11:00:13.648770Z","shell.execute_reply.started":"2025-10-23T11:00:13.644369Z","shell.execute_reply":"2025-10-23T11:00:13.648000Z"}},"outputs":[],"execution_count":63},{"cell_type":"code","source":"class FeedForward(nn.Module): \n    def __init__(self,cfg:dict): \n        super().__init__() \n        self.layers = nn.Sequential(\n            nn.Linear(cfg['emb_dim'],4*cfg['emb_dim']),\n            GELU(), \n            nn.Linear(4 * cfg['emb_dim'], cfg['emb_dim'])\n        )\n    \n    def forward(self,x): \n        return self.layers(x) \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T11:00:14.047831Z","iopub.execute_input":"2025-10-23T11:00:14.048105Z","iopub.status.idle":"2025-10-23T11:00:14.052567Z","shell.execute_reply.started":"2025-10-23T11:00:14.048086Z","shell.execute_reply":"2025-10-23T11:00:14.051849Z"}},"outputs":[],"execution_count":64},{"cell_type":"code","source":"class LayerNorm(nn.Module): \n    def __init__(self,emb_dim): \n        super().__init__() \n        self.eps = 1e-5 \n        self.scale = nn.Parameter(torch.ones(emb_dim)) \n        self.shift = nn.Parameter(torch.zeros(emb_dim)) \n    \n    def forward(self,x): \n        mean = x.mean(dim=-1,keepdim=True) \n        var = x.var(dim=-1,keepdim=True,unbiased=False) \n        norm_x  = (x - mean) / torch.sqrt(var + self.eps)\n        return self.scale * norm_x + self.shift # we are not forcing them to be gausian , model \n        # can do what it whant here :) \n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T11:00:15.567043Z","iopub.execute_input":"2025-10-23T11:00:15.567346Z","iopub.status.idle":"2025-10-23T11:00:15.572426Z","shell.execute_reply.started":"2025-10-23T11:00:15.567326Z","shell.execute_reply":"2025-10-23T11:00:15.571743Z"}},"outputs":[],"execution_count":65},{"cell_type":"code","source":"class TransformerBlock(nn.Module): \n    def __init__(self,cfg:dict):\n        super().__init__() \n        self.att = MultiHeadAttention(\n            d_in= cfg['emb_dim'],\n            d_out = cfg['emb_dim'], \n            context_length=cfg['context_length'], \n            num_heads = cfg['n_heads'], \n            dropout = cfg['drop_rate'], \n            qkv_bias=cfg['qkv_bias'] \n        )\n        self.ff = FeedForward(cfg) \n        self.norm1 = LayerNorm(cfg['emb_dim']) \n        self.norm2 = LayerNorm(cfg['emb_dim']) \n        self.drop_shortcut = nn.Dropout(cfg['drop_rate']) \n    \n    def forward(self,x): \n        shortcut = x \n        x = self.norm1(x) \n        x = self.att(x) \n        x = self.drop_shortcut(x) \n        x = x + shortcut \n\n        shortcut = x \n        x = self.norm2(x) \n        x = self.ff(x)  \n        x = self.drop_shortcut(x) \n        x = x + shortcut \n        return x \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T11:00:15.912373Z","iopub.execute_input":"2025-10-23T11:00:15.913071Z","iopub.status.idle":"2025-10-23T11:00:15.918610Z","shell.execute_reply.started":"2025-10-23T11:00:15.913046Z","shell.execute_reply":"2025-10-23T11:00:15.917911Z"}},"outputs":[],"execution_count":66},{"cell_type":"code","source":"def assign(left, right):\n    if left.shape != right.shape:\n        raise ValueError(f\"Shape mismatch. Left: {left.shape}, \"\n        \"Right: {right.shape}\"\n    )\n    return torch.nn.Parameter(torch.tensor(right))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T11:00:18.215153Z","iopub.execute_input":"2025-10-23T11:00:18.215754Z","iopub.status.idle":"2025-10-23T11:00:18.219637Z","shell.execute_reply.started":"2025-10-23T11:00:18.215729Z","shell.execute_reply":"2025-10-23T11:00:18.218850Z"}},"outputs":[],"execution_count":67},{"cell_type":"code","source":"class GPTModel(nn.Module): \n    def __init__(self,cfg:dict): \n        super().__init__() \n        self.tok_emb = nn.Embedding(cfg['vocab_size'],cfg['emb_dim']) \n        self.pos_emb = nn.Embedding(cfg['context_length'],cfg['emb_dim']) \n        self.drop_emb = nn.Dropout(cfg['drop_rate']) \n\n        self.trf_blocks = nn.Sequential(\n            *[TransformerBlock(cfg) for _ in range(cfg['n_layers'])]\n        )\n        self.final_norm = LayerNorm(cfg['emb_dim']) \n\n        self.out_head = nn.Linear( \n            cfg['emb_dim'], cfg['vocab_size'],bias=False\n        )\n    \n    def forward(self,in_idx:torch.Tensor): \n        batch_size, sq_len =  in_idx.shape \n        tok_embeds = self.tok_emb(in_idx)  \n        pos_embeds = self.pos_emb(\n            torch.arange(sq_len,device=in_idx.device)\n        )\n        x = tok_embeds + pos_embeds \n        x = self.drop_emb(x) \n        x = self.trf_blocks(x) \n        x = self.final_norm(x) \n        logits = self.out_head(x) \n        return logits ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T11:00:18.530645Z","iopub.execute_input":"2025-10-23T11:00:18.530906Z","iopub.status.idle":"2025-10-23T11:00:18.537054Z","shell.execute_reply.started":"2025-10-23T11:00:18.530885Z","shell.execute_reply":"2025-10-23T11:00:18.536300Z"}},"outputs":[],"execution_count":68},{"cell_type":"code","source":"def generate_text_simple(model,idx,max_new_tokens,context_size): \n    for _ in range(max_new_tokens):\n        idx_cond = idx[:,-context_size:] \n        with torch.no_grad(): \n            logits = model(idx_cond) \n        logits = logits[:,-1,:]\n        probas = torch.softmax(logits,dim=-1) \n        idx_next = torch.argmax(probas,dim=-1,keepdim=True) \n        idx = torch.cat((idx,idx_next),dim=1) \n    return idx ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T11:00:20.158597Z","iopub.execute_input":"2025-10-23T11:00:20.159391Z","iopub.status.idle":"2025-10-23T11:00:20.163802Z","shell.execute_reply.started":"2025-10-23T11:00:20.159359Z","shell.execute_reply":"2025-10-23T11:00:20.163112Z"}},"outputs":[],"execution_count":69},{"cell_type":"code","source":"import numpy as np\ndef load_weights_into_gpt(gpt, params):\n    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])\n    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])\n    for b in range(len(params[\"blocks\"])):\n        q_w, k_w, v_w = np.split(\n            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n        gpt.trf_blocks[b].att.W_query.weight = assign(\n            gpt.trf_blocks[b].att.W_query.weight, q_w.T)\n        gpt.trf_blocks[b].att.W_key.weight = assign(\n            gpt.trf_blocks[b].att.W_key.weight, k_w.T)\n        gpt.trf_blocks[b].att.W_value.weight = assign(\n            gpt.trf_blocks[b].att.W_value.weight, v_w.T)\n        q_b, k_b, v_b = np.split(\n            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n        gpt.trf_blocks[b].att.W_query.bias = assign(\n            gpt.trf_blocks[b].att.W_query.bias, q_b)\n        gpt.trf_blocks[b].att.W_key.bias = assign(\n            gpt.trf_blocks[b].att.W_key.bias, k_b)\n        gpt.trf_blocks[b].att.W_value.bias = assign(\n            gpt.trf_blocks[b].att.W_value.bias, v_b)\n        gpt.trf_blocks[b].att.out_proj.weight = assign(\n            gpt.trf_blocks[b].att.out_proj.weight,\n            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n        gpt.trf_blocks[b].att.out_proj.bias = assign(\n            gpt.trf_blocks[b].att.out_proj.bias,\n            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n        gpt.trf_blocks[b].ff.layers[0].weight = assign(\n            gpt.trf_blocks[b].ff.layers[0].weight,\n            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n        gpt.trf_blocks[b].ff.layers[0].bias = assign(\n            gpt.trf_blocks[b].ff.layers[0].bias,\n            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n        gpt.trf_blocks[b].ff.layers[2].weight = assign(\n            gpt.trf_blocks[b].ff.layers[2].weight,\n            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n        gpt.trf_blocks[b].ff.layers[2].bias = assign(\n            gpt.trf_blocks[b].ff.layers[2].bias,\n            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n        gpt.trf_blocks[b].norm1.scale = assign(\n            gpt.trf_blocks[b].norm1.scale,\n            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n        gpt.trf_blocks[b].norm1.shift = assign(\n            gpt.trf_blocks[b].norm1.shift,\n            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n        gpt.trf_blocks[b].norm2.scale = assign(\n            gpt.trf_blocks[b].norm2.scale,\n            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n        gpt.trf_blocks[b].norm2.shift = assign(\n            gpt.trf_blocks[b].norm2.shift,\n            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T11:00:20.456644Z","iopub.execute_input":"2025-10-23T11:00:20.456910Z","iopub.status.idle":"2025-10-23T11:00:20.469602Z","shell.execute_reply.started":"2025-10-23T11:00:20.456891Z","shell.execute_reply":"2025-10-23T11:00:20.468995Z"}},"outputs":[],"execution_count":70},{"cell_type":"code","source":"BASE_CONFIG = {\n\"vocab_size\": 50257,\n\"context_length\": 1024,\n\"drop_rate\": 0.0,\n\"qkv_bias\": True\n}\n# Vocabulary size\n# Context length\n# Dropout rate\n# Query-key-value bias\nmodel_configs = {\n\"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n\"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n\"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n\"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n}\nCHOOSE_MODEL = \"gpt2-medium (355M)\"\nBASE_CONFIG.update(model_configs[CHOOSE_MODEL])\nmodel_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\nsettings, params = download_and_load_gpt2(\nmodel_size=model_size,\nmodels_dir=\"gpt2\"\n)\nmodel = GPTModel(BASE_CONFIG)\nload_weights_into_gpt(model, params)\nmodel.eval();","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T11:00:23.056839Z","iopub.execute_input":"2025-10-23T11:00:23.057112Z","iopub.status.idle":"2025-10-23T11:00:30.186677Z","shell.execute_reply.started":"2025-10-23T11:00:23.057093Z","shell.execute_reply":"2025-10-23T11:00:30.185873Z"}},"outputs":[{"name":"stdout","text":"File already exists and is up-to-date: gpt2/355M/checkpoint\nFile already exists and is up-to-date: gpt2/355M/encoder.json\nFile already exists and is up-to-date: gpt2/355M/hparams.json\nFile already exists and is up-to-date: gpt2/355M/model.ckpt.data-00000-of-00001\nFile already exists and is up-to-date: gpt2/355M/model.ckpt.index\nFile already exists and is up-to-date: gpt2/355M/model.ckpt.meta\nFile already exists and is up-to-date: gpt2/355M/vocab.bpe\n","output_type":"stream"}],"execution_count":71},{"cell_type":"code","source":"torch.manual_seed(123)\ninput_text = format_input(val_data[0])\nprint(input_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T11:00:30.187789Z","iopub.execute_input":"2025-10-23T11:00:30.188016Z","iopub.status.idle":"2025-10-23T11:00:30.192805Z","shell.execute_reply.started":"2025-10-23T11:00:30.187991Z","shell.execute_reply":"2025-10-23T11:00:30.192278Z"}},"outputs":[{"name":"stdout","text":"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nConvert the active sentence to passive: 'The chef cooks the meal every day.'\n","output_type":"stream"}],"execution_count":72},{"cell_type":"code","source":"def text_to_token_ids(text,tokenizer): \n    encoded = tokenizer.encode(text,allowed_special={\"<|endoftext|>\"}) \n    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n    return encoded_tensor ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T11:00:30.193514Z","iopub.execute_input":"2025-10-23T11:00:30.193691Z","iopub.status.idle":"2025-10-23T11:00:30.204663Z","shell.execute_reply.started":"2025-10-23T11:00:30.193678Z","shell.execute_reply":"2025-10-23T11:00:30.204101Z"}},"outputs":[],"execution_count":73},{"cell_type":"code","source":"def token_ids_to_text(token_ids:torch.Tensor,tokenizer): \n    flat = token_ids.squeeze(0) # remove batch dimenstion \n    return tokenizer.decode(flat.tolist())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T11:00:30.206342Z","iopub.execute_input":"2025-10-23T11:00:30.206597Z","iopub.status.idle":"2025-10-23T11:00:30.216874Z","shell.execute_reply.started":"2025-10-23T11:00:30.206579Z","shell.execute_reply":"2025-10-23T11:00:30.216116Z"}},"outputs":[],"execution_count":74},{"cell_type":"code","source":"def generate(model,idx,max_new_tokens,context_size,temperature=0.0,top_k=None,eos_id=None):\n    for _ in range(max_new_tokens): \n        idx_cond = idx[:,-context_size:]  \n        with torch.no_grad(): \n            logits = model(idx_cond) \n        \n        logits = logits[:,-1,:]\n\n        if top_k is not None: \n            top_logits,_ = torch.topk(logits,top_k) \n            min_val = top_logits[:,-1] \n            logits = torch.where(\n                logits < min_val, \n                torch.tensor(float('-inf')).to(logits.device), \n                logits \n            )\n        if temperature > 0.0: \n            logits = logits / temperature # :) \n            probs  = torch.softmax(logits,dim=-1) \n            idx_next = torch.multinomial(probs,num_samples=1)\n        else: \n            idx_next = torch.argmax(logits,dim=-1,keepdim=True)\n        if idx_next == eos_id: \n            break \n        idx = torch.cat((idx,idx_next),dim=1) # C \n    return idx \n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T11:00:30.217722Z","iopub.execute_input":"2025-10-23T11:00:30.218302Z","iopub.status.idle":"2025-10-23T11:00:30.229852Z","shell.execute_reply.started":"2025-10-23T11:00:30.218275Z","shell.execute_reply":"2025-10-23T11:00:30.229189Z"}},"outputs":[],"execution_count":75},{"cell_type":"code","source":"def evaluate_model(model,train_loader,val_loader,device,eval_iter): \n    model.eval() \n    with torch.inference_mode(): # modern \n        train_loss = calc_loss_loader(\n            train_loader,model,device,num_batches=eval_iter\n        )\n        val_loss = calc_loss_loader(\n            val_loader,model,device,num_batches=eval_iter\n        )\n    model.train() # turn back model to train mode \n    return train_loss, val_loss ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T11:00:30.230468Z","iopub.execute_input":"2025-10-23T11:00:30.230694Z","iopub.status.idle":"2025-10-23T11:00:30.244756Z","shell.execute_reply.started":"2025-10-23T11:00:30.230678Z","shell.execute_reply":"2025-10-23T11:00:30.244173Z"}},"outputs":[],"execution_count":76},{"cell_type":"code","source":"token_ids = generate(\nmodel=model,\nidx=text_to_token_ids(input_text, tokenizer),\nmax_new_tokens=35,\ncontext_size=BASE_CONFIG[\"context_length\"],\neos_id=50256,\n)\ngenerated_text = token_ids_to_text(token_ids, tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T11:00:30.245407Z","iopub.execute_input":"2025-10-23T11:00:30.245618Z","iopub.status.idle":"2025-10-23T11:00:43.444880Z","shell.execute_reply.started":"2025-10-23T11:00:30.245597Z","shell.execute_reply":"2025-10-23T11:00:43.444063Z"}},"outputs":[],"execution_count":77},{"cell_type":"code","source":"response_text = generated_text[len(input_text):].strip()\nprint(response_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T11:00:43.445720Z","iopub.execute_input":"2025-10-23T11:00:43.445941Z","iopub.status.idle":"2025-10-23T11:00:43.450986Z","shell.execute_reply.started":"2025-10-23T11:00:43.445918Z","shell.execute_reply":"2025-10-23T11:00:43.450226Z"}},"outputs":[{"name":"stdout","text":"### Response:\n\nThe chef cooks the meal every day.\n\n### Instruction:\n\nConvert the active sentence to passive: 'The chef cooks the\n","output_type":"stream"}],"execution_count":78},{"cell_type":"code","source":"def calc_loss_batch(input_batch,target_batch,model,device): \n    input_batch = input_batch.to(device) \n    target_batch = target_batch.to(device) \n    logits = model(input_batch)\n    loss = torch.nn.functional.cross_entropy(\n        logits.flatten(0,1),target_batch.flatten()\n    )\n    return loss ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T11:00:43.451767Z","iopub.execute_input":"2025-10-23T11:00:43.452001Z","iopub.status.idle":"2025-10-23T11:00:43.465768Z","shell.execute_reply.started":"2025-10-23T11:00:43.451971Z","shell.execute_reply":"2025-10-23T11:00:43.465166Z"}},"outputs":[],"execution_count":79},{"cell_type":"code","source":"# Fine-tuning the LLM on instruction data\ndef calc_loss_loader(data_loader,model,device,num_batches=None): \n    total_loss = 0 \n    if len(data_loader) == 0: \n        return float(\"nan\") \n    elif num_batches is None: \n        num_batches = len(data_loader) \n    else: \n        num_batches = min(num_batches,len(data_loader)) \n    \n    for i ,(input_batch,target_batch) in enumerate(data_loader): \n        if i < num_batches: \n            loss = calc_loss_batch(\n                input_batch,target_batch,model,device\n            ) \n            total_loss += loss.item() \n        else: \n            break \n\n    return total_loss / num_batches ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T11:00:43.467550Z","iopub.execute_input":"2025-10-23T11:00:43.467759Z","iopub.status.idle":"2025-10-23T11:00:43.478567Z","shell.execute_reply.started":"2025-10-23T11:00:43.467744Z","shell.execute_reply":"2025-10-23T11:00:43.477935Z"}},"outputs":[],"execution_count":80},{"cell_type":"code","source":"# Training an LLM \ndef train_model_simple(model,train_loader,val_loader,optimizer,device,num_epochs,\n                        eval_freq,eval_iter,start_context,tokenizer): \n                        print(\"DEVICE:\",device)\n                        model.to(device)\n                        train_loader = train_loader.to(device)\n                        val_loader  = val_loader.to(device)\n                        try:\n                            optimizer = optimizer.to(device)\n                        except:\n                            print(\":(, you dont undersntand pytorch\")\n                        \n                        #---------------------------------\n                        train_losses , val_losses, track_tokens_seen = [],[], [] \n                        tokens_seen, gobal_step = 0 , -1 \n                        for epoch in range(num_epochs): \n                            model.train() # model to train model  \n                            for input_batch,target_batch in train_loader: \n                                optimizer.zero_grad() \n                                loss = calc_loss_batch(\n                                    input_batch,target_batch,model,device\n                                )   \n                                loss.backward() \n                                optimizer.step() \n                                tokens_seen += input_batch.numel() \n                                gobal_step += 1 \n\n                                if gobal_step % eval_freq == 0: \n                                    train_loss, val_loss = evaluate_model(\n                                        model, train_loader,val_loader,device,eval_iter\n                                    )  \n\n                                    train_losses.append(train_loss) \n                                    val_losses.append(val_loss) \n                                    track_tokens_seen.append(tokens_seen) \n                                    print(\n                                        f\"EP {epoch +1} ( Step {global_step:06d}):\"\n                                        f\"Train loss {train_loss:.3f},\"\n                                        f\"Val loss {val_loss:.3f}\"\n                                    )          \n                            generate_and_print_sample(\n                                model,tokenizer,device,start_context\n                            )\n                        \n                        return train_loss, val_loss, track_tokens_seen \n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T11:08:19.062521Z","iopub.execute_input":"2025-10-23T11:08:19.063049Z","iopub.status.idle":"2025-10-23T11:08:19.069711Z","shell.execute_reply.started":"2025-10-23T11:08:19.063026Z","shell.execute_reply":"2025-10-23T11:08:19.068938Z"}},"outputs":[],"execution_count":92},{"cell_type":"code","source":"model.to(device)\ntorch.manual_seed(123)\nwith torch.no_grad():\n    train_loss = calc_loss_loader(\n    train_loader, model, device, num_batches=5\n    )\n    val_loss = calc_loss_loader(\n    val_loader, model, device, num_batches=5\n    )\nprint(\"Training loss:\", train_loss)\nprint(\"Validation loss:\", val_loss)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T11:09:24.921626Z","iopub.execute_input":"2025-10-23T11:09:24.922356Z","iopub.status.idle":"2025-10-23T11:09:24.945173Z","shell.execute_reply.started":"2025-10-23T11:09:24.922330Z","shell.execute_reply":"2025-10-23T11:09:24.944158Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_37/3058372926.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m123\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     train_loss = calc_loss_loader(\n\u001b[1;32m      5\u001b[0m     \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_batches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1341\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1343\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1345\u001b[0m     def register_full_backward_pre_hook(\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    928\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m             \u001b[0mp_should_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1327\u001b[0m                         \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m                     )\n\u001b[0;32m-> 1329\u001b[0;31m                 return t.to(\n\u001b[0m\u001b[1;32m   1330\u001b[0m                     \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m                     \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 198.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 205.12 MiB is free. Process 5610 has 15.69 GiB memory in use. Of the allocated memory 15.25 GiB is allocated by PyTorch, and 144.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 198.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 205.12 MiB is free. Process 5610 has 15.69 GiB memory in use. Of the allocated memory 15.25 GiB is allocated by PyTorch, and 144.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","output_type":"error"}],"execution_count":99},{"cell_type":"code","source":"import time\nstart_time = time.time()\ntorch.manual_seed(123)\noptimizer = torch.optim.AdamW(\nmodel.parameters(), lr=0.00005, weight_decay=0.1\n)\nnum_epochs = 2\ntrain_losses, val_losses, tokens_seen = train_model_simple(\nmodel, train_loader, val_loader, optimizer, device,\nnum_epochs=num_epochs, eval_freq=5, eval_iter=5,\nstart_context=format_input(val_data[0]), tokenizer=tokenizer\n)\nend_time = time.time()\nexecution_time_minutes = (end_time - start_time) / 60\nprint(f\"Training completed in {execution_time_minutes:.2f} minutes.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T11:08:44.444224Z","iopub.execute_input":"2025-10-23T11:08:44.444513Z","iopub.status.idle":"2025-10-23T11:08:44.480694Z","shell.execute_reply.started":"2025-10-23T11:08:44.444492Z","shell.execute_reply":"2025-10-23T11:08:44.479648Z"}},"outputs":[{"name":"stdout","text":"DEVICE: cuda\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_37/2985545885.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      7\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m train_losses, val_losses, tokens_seen = train_model_simple(\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_37/1215400089.py\u001b[0m in \u001b[0;36mtrain_model_simple\u001b[0;34m(model, train_loader, val_loader, optimizer, device, num_epochs, eval_freq, eval_iter, start_context, tokenizer)\u001b[0m\n\u001b[1;32m      3\u001b[0m                         eval_freq,eval_iter,start_context,tokenizer): \n\u001b[1;32m      4\u001b[0m                         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"DEVICE:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m                         \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                         \u001b[0mval_loader\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1341\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1343\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1345\u001b[0m     def register_full_backward_pre_hook(\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    928\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m             \u001b[0mp_should_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1327\u001b[0m                         \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m                     )\n\u001b[0;32m-> 1329\u001b[0;31m                 return t.to(\n\u001b[0m\u001b[1;32m   1330\u001b[0m                     \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m                     \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 198.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 205.12 MiB is free. Process 5610 has 15.69 GiB memory in use. Of the allocated memory 15.25 GiB is allocated by PyTorch, and 144.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 198.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 205.12 MiB is free. Process 5610 has 15.69 GiB memory in use. Of the allocated memory 15.25 GiB is allocated by PyTorch, and 144.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","output_type":"error"}],"execution_count":98},{"cell_type":"code","source":"torch.cuda.empty_cache() # clearn memory","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T11:08:39.970631Z","iopub.execute_input":"2025-10-23T11:08:39.971167Z","iopub.status.idle":"2025-10-23T11:08:39.974643Z","shell.execute_reply.started":"2025-10-23T11:08:39.971145Z","shell.execute_reply":"2025-10-23T11:08:39.973921Z"}},"outputs":[],"execution_count":96},{"cell_type":"code","source":"print(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T11:08:40.857760Z","iopub.execute_input":"2025-10-23T11:08:40.858026Z","iopub.status.idle":"2025-10-23T11:08:40.862067Z","shell.execute_reply.started":"2025-10-23T11:08:40.858006Z","shell.execute_reply":"2025-10-23T11:08:40.861419Z"}},"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}],"execution_count":97},{"cell_type":"code","source":"# continue... ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}