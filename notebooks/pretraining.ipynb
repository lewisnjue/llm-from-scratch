{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "796783a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c84c0c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "\"vocab_size\": 50257,\n",
    "\"context_length\": 256, # i have reduced context lenght \n",
    "\"emb_dim\": 768,\n",
    "\"n_heads\": 12,\n",
    "\"n_layers\": 12,\n",
    "\"drop_rate\": 0.1,\n",
    "\"qkv_bias\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83882edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "        self.register_buffer(\"mask\", mask)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, D_in = x.shape\n",
    "\n",
    "        Q = self.W_query(x)  # (B, T, D_out)\n",
    "        K = self.W_key(x)    # (B, T, D_out)\n",
    "        V = self.W_value(x)  # (B, T, D_out)\n",
    "\n",
    "\n",
    "        Q = Q.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)  # (B, num_heads, T, head_dim)\n",
    "        K = K.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = V.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # Attention scores\n",
    "        attn_scores = (Q @ K.transpose(-2, -1)) / (self.head_dim ** 0.5)  # (B, num_heads, T, T)\n",
    "\n",
    "        mask = self.mask[:T, :T].bool()\n",
    "        attn_scores = attn_scores.masked_fill(mask[None, None, :, :], float('-inf'))\n",
    "\n",
    "        # Softmax and dropout\n",
    "        attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Apply attention\n",
    "        context = attn_weights @ V  # (B, num_heads, T, head_dim)\n",
    "\n",
    "        # Merge heads back\n",
    "        context = context.transpose(1, 2).contiguous().view(B, T, self.d_out)\n",
    "        context = self.out_proj(context)\n",
    "\n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a473b21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "        torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
    "        (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8235d37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module): \n",
    "    def __init__(self,cfg:dict): \n",
    "        super().__init__() \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg['emb_dim'],4*cfg['emb_dim']),\n",
    "            GELU(), \n",
    "            nn.Linear(4 * cfg['emb_dim'], cfg['emb_dim'])\n",
    "        )\n",
    "    \n",
    "    def forward(self,x): \n",
    "        return self.layers(x) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c8e6866",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module): \n",
    "    def __init__(self,emb_dim): \n",
    "        super().__init__() \n",
    "        self.eps = 1e-5 \n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim)) \n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim)) \n",
    "    \n",
    "    def forward(self,x): \n",
    "        mean = x.mean(dim=-1,keepdim=True) \n",
    "        var = x.var(dim=-1,keepdim=True,unbiased=False) \n",
    "        norm_x  = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift # we are not forcing them to be gausian , model \n",
    "        # can do what it whant here :) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "390a9723",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module): \n",
    "    def __init__(self,cfg:dict):\n",
    "        super().__init__() \n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in= cfg['emb_dim'],\n",
    "            d_out = cfg['emb_dim'], \n",
    "            context_length=cfg['context_length'], \n",
    "            num_heads = cfg['n_heads'], \n",
    "            dropout = cfg['drop_rate'], \n",
    "            qkv_bias=cfg['qkv_bias'] \n",
    "        )\n",
    "        self.ff = FeedForward(cfg) \n",
    "        self.norm1 = LayerNorm(cfg['emb_dim']) \n",
    "        self.norm2 = LayerNorm(cfg['emb_dim']) \n",
    "        self.drop_shortcut = nn.Dropout(cfg['drop_rate']) \n",
    "    \n",
    "    def forward(self,x): \n",
    "        shortcut = x \n",
    "        x = self.norm1(x) \n",
    "        x = self.att(x) \n",
    "        x = self.drop_shortcut(x) \n",
    "        x = x + shortcut \n",
    "\n",
    "        shortcut = x \n",
    "        x = self.norm2(x) \n",
    "        x = self.ff(x)  \n",
    "        x = self.drop_shortcut(x) \n",
    "        x = x + shortcut \n",
    "        return x \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "928b9c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module): \n",
    "    def __init__(self,cfg:dict): \n",
    "        super().__init__() \n",
    "        self.tok_emb = nn.Embedding(cfg['vocab_size'],cfg['emb_dim']) \n",
    "        self.pos_emb = nn.Embedding(cfg['context_length'],cfg['emb_dim']) \n",
    "        self.drop_emb = nn.Dropout(cfg['drop_rate']) \n",
    "\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg['n_layers'])]\n",
    "        )\n",
    "        self.final_norm = LayerNorm(cfg['emb_dim']) \n",
    "\n",
    "        self.out_head = nn.Linear( \n",
    "            cfg['emb_dim'], cfg['vocab_size'],bias=False\n",
    "        )\n",
    "    \n",
    "    def forward(self,in_idx:torch.Tensor): \n",
    "        batch_size, sq_len =  in_idx.shape \n",
    "        tok_embeds = self.tok_emb(in_idx)  \n",
    "        pos_embeds = self.pos_emb(\n",
    "            torch.arange(sq_len,device=in_idx.device)\n",
    "        )\n",
    "        x = tok_embeds + pos_embeds \n",
    "        x = self.drop_emb(x) \n",
    "        x = self.trf_blocks(x) \n",
    "        x = self.final_norm(x) \n",
    "        logits = self.out_head(x) \n",
    "        return logits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "97e9ea7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model,idx,max_new_tokens,context_size): \n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:,-context_size:] \n",
    "        with torch.no_grad(): \n",
    "            logits = model(idx_cond) \n",
    "        logits = logits[:,-1,:]\n",
    "        probas = torch.softmax(logits,dim=-1) \n",
    "        idx_next = torch.argmax(probas,dim=-1,keepdim=True) \n",
    "        idx = torch.cat((idx,idx_next),dim=1) \n",
    "    return idx "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9e272756",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "65dae294",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d8d0d5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_token(text,tokenizer): \n",
    "    encoded = tokenizer.encode(text,allowed_special={\"<|endoftext|>\"}) \n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
    "    return encoded_tensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4ba112ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_ids_to_text(token_ids:torch.Tensor,tokenizer): \n",
    "    flat = token_ids.squeeze(0) # remove batch dimenstion \n",
    "    return tokenizer.decode(flat.tolist())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c5a915b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you rentingetic wasnم refres RexMeCHicular stren\n"
     ]
    }
   ],
   "source": [
    "start_context = \"Every effort moves you\" \n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\") \n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token(start_context,tokenizer), \n",
    "    max_new_tokens=10, \n",
    "    context_size = GPT_CONFIG_124M['context_length']\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\",token_ids_to_text(token_ids,tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "739a622a",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[16833, 3626, 6100],\n",
    "[40,\n",
    "1107, 588]])\n",
    "# [\"every effort moves\",\n",
    "# \"I really like\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a490060b",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = torch.tensor([[3626, 6100, 345 ],\n",
    "[1107, 588, 11311]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9203442d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 50257])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad(): \n",
    "    logits = model(inputs) \n",
    "\n",
    "probas = torch.softmax(logits,dim=-1) \n",
    "print(probas.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7a71c5d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[[16657],\n",
      "         [  339],\n",
      "         [42826]],\n",
      "\n",
      "        [[49906],\n",
      "         [29669],\n",
      "         [41751]]])\n",
      "torch.Size([2, 3, 1])\n"
     ]
    }
   ],
   "source": [
    "token_ids = torch.argmax(probas,dim=-1,keepdim=True) \n",
    "print(\"Token IDs:\\n\",token_ids) \n",
    "print(token_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "faf539b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets batch 1:  effort moves you\n",
      "Outputs batch 1:  Armed heNetflix\n"
     ]
    }
   ],
   "source": [
    "print(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\n",
    "print(f\"Outputs batch 1:\"\n",
    "f\" {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "308d5db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1: tensor([7.4540e-05, 3.1061e-05, 1.1563e-05])\n",
      "Text 2: tensor([1.0337e-05, 5.6776e-05, 4.7559e-06])\n"
     ]
    }
   ],
   "source": [
    "text_idx = 0\n",
    "target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 1:\", target_probas_1)\n",
    "text_idx = 1\n",
    "target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 2:\", target_probas_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5540ce1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ -9.5042, -10.3796, -11.3677, -11.4798,  -9.7764, -12.2561])\n"
     ]
    }
   ],
   "source": [
    "log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\n",
    "print(log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "27b1fb4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-10.7940)\n",
      "tensor(10.7940)\n"
     ]
    }
   ],
   "source": [
    "avg_log_probas = torch.mean(log_probas)\n",
    "print(avg_log_probas) # -loss , loss = -avg_log_probs \n",
    "\n",
    "print(-avg_log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "adfb4276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.7940)\n"
     ]
    }
   ],
   "source": [
    "neg_avg_log_probas = avg_log_probas * -1\n",
    "print(neg_avg_log_probas) # this is known as cross entropy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b7e6fc63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([2, 3, 50257])\n",
      "Targets shape: torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "print(\"Logits shape:\", logits.shape)\n",
    "print(\"Targets shape:\", targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a9f7c716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened logits: torch.Size([6, 50257])\n",
      "Flattened targets: torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "# before using cross entropy you need to flatten  on batch and time dimention like this \n",
    "logits_flat = logits.flatten(0, 1)\n",
    "targets_flat = targets.flatten()\n",
    "print(\"Flattened logits:\", logits_flat.shape)\n",
    "print(\"Flattened targets:\", targets_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1324b203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.7940)\n"
     ]
    }
   ],
   "source": [
    "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4b6187",
   "metadata": {},
   "source": [
    "perplexity is a measure often used along side cross entropy loss to evaluate the performance of models in tasks like lagunage modeling. it can provide a more interpretable way to understand the uncertainty of a model in predicting the next token in a sequence. \n",
    "perplexity measures how well the probability distribution predicted by the mode matches the actual distribution of the words in a dataset. Similar to the loss , a lower perplexity indicates the model predictions are closer to the actual distribution . Perplexity can be cacluated as ` perplexity = torch.exp(loss)` which returns tensor(48725.8203) when applied to the previously caclated loss. \n",
    "\n",
    "perplexity is often considered more interpretable than the raw loss value because it signifies teh effective vocabulary size about which the model is uncertain at each step. In the given exampe , this would translate to the model being unsure about which among 48,725 tokens in the vocabulary to generate as the next token "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e65f0157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will apply the loss computation to the entire training and validation sets. \n",
    "file_path = \"../the-verdict.txt\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    text_data = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7dc93ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: 20479\n",
      "Tokens: 5145\n"
     ]
    }
   ],
   "source": [
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "print(\"Characters:\", total_characters)\n",
    "print(\"Tokens:\", total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "62d61217",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6e7f5e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9ee93535",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTDatasetV1(Dataset): \n",
    "    def __init__(self,txt,tokenizer,max_length,stride): \n",
    "        self.input_ids = [] \n",
    "        self.target_ids = []\n",
    "\n",
    "        token_ids = tokenizer.encode(txt) \n",
    "\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "        \n",
    "    def __len__(self): \n",
    "        return len(self.input_ids) \n",
    "    \n",
    "    def __getitem__(self,idx): \n",
    "        return self.input_ids[idx] , self.target_ids[idx] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ce5f05ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
    "                    stride=128, shuffle=True, drop_last=True,\n",
    "                    num_workers=0):\n",
    "                    \n",
    "                    tokenizer  = tiktoken.get_encoding('gpt2') \n",
    "                    dataset = GPTDatasetV1(txt,tokenizer,max_length,stride) \n",
    "                    dataloader = DataLoader(\n",
    "                        dataset, \n",
    "                        batch_size=batch_size, \n",
    "                        shuffle=shuffle, \n",
    "                        drop_last=drop_last, \n",
    "                        num_workers=num_workers\n",
    "                    )\n",
    "                    return dataloader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f0faef41",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = create_dataloader_v1(\n",
    "train_data,\n",
    "batch_size=2,\n",
    "max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "drop_last=True,\n",
    "shuffle=True,\n",
    "num_workers=0\n",
    ")\n",
    "val_loader = create_dataloader_v1(\n",
    "val_data,\n",
    "batch_size=2,\n",
    "max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "drop_last=False,\n",
    "shuffle=False,\n",
    "num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ab2dc371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "\n",
      "Validation loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Train loader:\")\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "print(\"\\nValidation loader:\")\n",
    "for x, y in val_loader:\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cccf76f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch,target_batch,model,device): \n",
    "    input_batch = input_batch.to(device) \n",
    "    target_batch = target_batch.to(device) \n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(\n",
    "        logits.flatten(0,1),target_batch.flatten()\n",
    "    )\n",
    "    return loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8465ec2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_loader(data_loader,model,device,num_batches=None): \n",
    "    total_loss = 0 \n",
    "    if len(data_loader) == 0: \n",
    "        return float(\"nan\") \n",
    "    elif num_batches is None: \n",
    "        num_batches = len(data_loader) \n",
    "    else: \n",
    "        num_batches = min(num_batches,len(data_loader)) \n",
    "    \n",
    "    for i ,(input_batch,target_batch) in enumerate(data_loader): \n",
    "        if i < num_batches: \n",
    "            loss = calc_loss_batch(\n",
    "                input_batch,target_batch,model,device\n",
    "            ) \n",
    "            total_loss += loss.item() \n",
    "        else: \n",
    "            break \n",
    "\n",
    "    return total_loss / num_batches "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7fc3c929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss 10.987583266364204\n",
      " Validation loss 10.98110580444336\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else \"cpu\")\n",
    "model.eval() # model to eval mode \n",
    "\n",
    "with torch.inference_mode(): # more modern and potentaly fast\n",
    "    train_loss = calc_loss_loader(train_loader,model,device) \n",
    "    val_loss = calc_loss_loader(val_loader,model,device) \n",
    "\n",
    "print(\"Training loss\",train_loss) \n",
    "print(\" Validation loss\",val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3dfa2343",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model,train_loader,val_loader,device,eval_iter): \n",
    "    model.eval() \n",
    "    with torch.inference_mode(): # modern \n",
    "        train_loss = calc_loss_loader(\n",
    "            train_loader,model,device,num_batches=eval_iter\n",
    "        )\n",
    "        val_loss = calc_loss_loader(\n",
    "            val_loader,model,device,num_batches=eval_iter\n",
    "        )\n",
    "    model.train() # turn back model to train mode \n",
    "    return train_loss, val_loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "303d9151",
   "metadata": {},
   "outputs": [],
   "source": [
    "from base64 import decode\n",
    "\n",
    "\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval() # evaluation mode \n",
    "    context_size = model.pos_emb.weight.shape[0] \n",
    "    encoded = text_to_token(start_context,tokenizer).to(device) \n",
    "    with torch.no_grad(): \n",
    "        token_ids = generate_text_simple(\n",
    "            model=model,idx=encoded,max_new_tokens=50,context_size=context_size\n",
    "        )\n",
    "    \n",
    "    decoded_text = token_ids_to_text(token_ids,tokenizer) \n",
    "    print(decoded_text.replace(\"\\n\",\"\"))\n",
    "    model.train # back to train mode "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "daa21a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training an LLM \n",
    "def train_model_simple(model,train_loader,val_loader,optimizer,device,num_epochs,\n",
    "                        eval_freq,eval_iter,start_context,tokenizer): \n",
    "                        #---------------------------------\n",
    "                        train_losses , val_losses, track_tokens_seen = [],[], [] \n",
    "                        tokens_seen, global_step = 0 , -1 \n",
    "                        for epoch in range(num_epochs): \n",
    "                            model.train() # model to train model  \n",
    "                            for input_batch,target_batch in train_loader: \n",
    "                                optimizer.zero_grad() \n",
    "                                loss = calc_loss_batch(\n",
    "                                    input_batch,target_batch,model,device\n",
    "                                )   \n",
    "                                loss.backward() \n",
    "                                optimizer.step() \n",
    "                                tokens_seen += input_batch.numel() \n",
    "                                gobal_step += 1 \n",
    "\n",
    "                                if global_step % eval_freq == 0: \n",
    "                                    train_loss, val_loss = evaluate_model(\n",
    "                                        model, train_loader,val_loader,device,eval_iter\n",
    "                                    )  \n",
    "\n",
    "                                    train_losses.append(train_loss) \n",
    "                                    val_losses.append(val_loss) \n",
    "                                    track_tokens_seen.append(tokens_seen) \n",
    "                                    print(\n",
    "                                        f\"EP {epoch +1} ( Step {global_step:06d}):\"\n",
    "                                        f\"Train loss {train_loss:.3f},\"\n",
    "                                        f\"Val loss {val_loss:.3f}\"\n",
    "                                    )          \n",
    "                            generate_and_print_sample(\n",
    "                                model,tokenizer,device,start_context\n",
    "                            )\n",
    "                        \n",
    "                        return train_loss, val_loss, track_tokens_seen \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba3d628",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'torch._dynamo' from '/home/njue/Dev/LLMFromScratch/.venv/lib/python3.13/site-packages/torch/_dynamo/__init__.py' has no attribute 'decorators' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[71]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     15\u001b[39m model.to(device) \u001b[38;5;66;03m# :( \u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# model.compile() # :)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m optimizer = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptim\u001b[49m\u001b[43m.\u001b[49m\u001b[43mAdamW\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.0004\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m \u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m num_epochs = \u001b[32m10\u001b[39m \n\u001b[32m     23\u001b[39m train_losses, val_losses , token_seen = train_model_simple(\n\u001b[32m     24\u001b[39m     model,train_loader,val_loader,optimizer,device,num_epochs,eval_freq=\u001b[32m5\u001b[39m,eval_iter=\u001b[32m5\u001b[39m, \n\u001b[32m     25\u001b[39m     start_context=\u001b[33m\"\u001b[39m\u001b[33mEvery effort moves you\u001b[39m\u001b[33m\"\u001b[39m, tokenizer=tokenizer\n\u001b[32m     26\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/LLMFromScratch/.venv/lib/python3.13/site-packages/torch/optim/adamw.py:37\u001b[39m, in \u001b[36mAdamW.__init__\u001b[39m\u001b[34m(self, params, lr, betas, eps, weight_decay, amsgrad, maximize, foreach, capturable, differentiable, fused)\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m     23\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     24\u001b[39m     params: ParamsT,\n\u001b[32m   (...)\u001b[39m\u001b[32m     35\u001b[39m     fused: Optional[\u001b[38;5;28mbool\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     36\u001b[39m ):\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbetas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforeach\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/LLMFromScratch/.venv/lib/python3.13/site-packages/torch/optim/adam.py:101\u001b[39m, in \u001b[36mAdam.__init__\u001b[39m\u001b[34m(self, params, lr, betas, eps, weight_decay, amsgrad, foreach, maximize, capturable, differentiable, fused, decoupled_weight_decay)\u001b[39m\n\u001b[32m     86\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mTensor betas[1] must be 1-element\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     88\u001b[39m defaults = {\n\u001b[32m     89\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mlr\u001b[39m\u001b[33m\"\u001b[39m: lr,\n\u001b[32m     90\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mbetas\u001b[39m\u001b[33m\"\u001b[39m: betas,\n\u001b[32m   (...)\u001b[39m\u001b[32m     99\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mdecoupled_weight_decay\u001b[39m\u001b[33m\"\u001b[39m: decoupled_weight_decay,\n\u001b[32m    100\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefaults\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fused:\n\u001b[32m    104\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m differentiable:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/LLMFromScratch/.venv/lib/python3.13/site-packages/torch/optim/optimizer.py:401\u001b[39m, in \u001b[36mOptimizer.__init__\u001b[39m\u001b[34m(self, params, defaults)\u001b[39m\n\u001b[32m    398\u001b[39m     param_groups = [{\u001b[33m\"\u001b[39m\u001b[33mparams\u001b[39m\u001b[33m\"\u001b[39m: param_groups}]\n\u001b[32m    400\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m param_group \u001b[38;5;129;01min\u001b[39;00m param_groups:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madd_param_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_group\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[38;5;66;03m# Allows _cuda_graph_capture_health_check to rig a poor man's TORCH_WARN_ONCE in python,\u001b[39;00m\n\u001b[32m    404\u001b[39m \u001b[38;5;66;03m# which I don't think exists\u001b[39;00m\n\u001b[32m    405\u001b[39m \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/issues/72948\u001b[39;00m\n\u001b[32m    406\u001b[39m \u001b[38;5;28mself\u001b[39m._warned_capturable_if_run_uncaptured = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/LLMFromScratch/.venv/lib/python3.13/site-packages/torch/_compile.py:46\u001b[39m, in \u001b[36m_disable_dynamo.<locals>.inner\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     44\u001b[39m disable_fn = \u001b[38;5;28mgetattr\u001b[39m(fn, \u001b[33m\"\u001b[39m\u001b[33m__dynamo_disable\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m disable_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_dynamo\u001b[39;00m\n\u001b[32m     48\u001b[39m     \u001b[38;5;66;03m# We can safely turn off functools.wraps here because the inner\u001b[39;00m\n\u001b[32m     49\u001b[39m     \u001b[38;5;66;03m# already wraps fn in the outer scope.\u001b[39;00m\n\u001b[32m     50\u001b[39m     disable_fn = torch._dynamo.disable(fn, recursive, wrapping=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/LLMFromScratch/.venv/lib/python3.13/site-packages/torch/_dynamo/__init__.py:107\u001b[39m\n\u001b[32m     73\u001b[39m __all__ = [\n\u001b[32m     74\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_in_graph\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     75\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33massume_constant_result\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    103\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33msubstitute_in_graph\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    104\u001b[39m ]\n\u001b[32m    106\u001b[39m \u001b[38;5;66;03m# allowlist this for weights_only load of NJTs\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m107\u001b[39m torch.serialization.add_safe_globals([\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_dynamo\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecorators\u001b[49m._DimRange])\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.manual_seed \u001b[38;5;129;01mis\u001b[39;00m torch.random.manual_seed:\n\u001b[32m    110\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mjit\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_builtins\u001b[39;00m\n",
      "\u001b[31mAttributeError\u001b[39m: partially initialized module 'torch._dynamo' from '/home/njue/Dev/LLMFromScratch/.venv/lib/python3.13/site-packages/torch/_dynamo/__init__.py' has no attribute 'decorators' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "Adam optimizers are a popular choice for training deep neural networks. However, in\n",
    "our training loop, we opt for the AdamW optimizer. AdamW is a variant of Adam that\n",
    "improves the weight decay approach, which aims to minimize model complexity and\n",
    "prevent overfitting by penalizing larger weights. This adjustment allows AdamW to\n",
    "achieve more effective regularization and better generalization; thus, AdamW is fre-\n",
    "quently used in the training of LLMs\n",
    "\"\"\" \n",
    "\n",
    "import token\n",
    "\n",
    "\n",
    "torch.manual_seed(123) \n",
    "model = GPTModel(GPT_CONFIG_124M) \n",
    "model.to(device) # :( \n",
    "# model.compile() # :)\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(), \n",
    "    lr = 0.0004 , \n",
    "    weight_decay=0.1 \n",
    ")\n",
    "num_epochs = 10 \n",
    "train_losses, val_losses , token_seen = train_model_simple(\n",
    "    model,train_loader,val_loader,optimizer,device,num_epochs,eval_freq=5,eval_iter=5, \n",
    "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
    ") # thier is a problem with my env i will fix later "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2b8d1afe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.9.0+cpu'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a3965847",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_epochs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[73]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     18\u001b[39m     fig.tight_layout()\n\u001b[32m     19\u001b[39m     plt.show()\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m epochs_tensor = torch.linspace(\u001b[32m0\u001b[39m, \u001b[43mnum_epochs\u001b[49m, \u001b[38;5;28mlen\u001b[39m(train_losses))\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m#plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'num_epochs' is not defined"
     ]
    }
   ],
   "source": [
    "# plotting code \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(\n",
    "    epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\"\n",
    "    )\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    ax2 = ax1.twiny()\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "#plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "703f31b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.to('cpu') \n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3a4cdf2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output text: \n",
      " every effort moves you rentingetic minion mobilized Macicone heterogeneity\u0000achaRAMlevision TT TelegramCustomer connector commander Mobil RBI transpiredestine formulations32 stereo vocabularyLD\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\") \n",
    "token_ids = generate_text_simple(\n",
    "    model = model, \n",
    "    idx = text_to_token(\"every effort moves you\",tokenizer), \n",
    "    max_new_tokens=25, \n",
    "    context_size=GPT_CONFIG_124M['context_length']\n",
    ")\n",
    "print(\"output text: \\n\",token_ids_to_text(token_ids,tokenizer)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ff47cd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temperature scaling \n",
    "vocab = {\n",
    "\"closer\": 0,\n",
    "\"every\": 1,\n",
    "\"effort\": 2,\n",
    "\"forward\": 3,\n",
    "\"inches\": 4,\n",
    "\"moves\": 5,\n",
    "\"pizza\": 6,\n",
    "\"toward\": 7,\n",
    "\"you\": 8,\n",
    "}\n",
    "inverse_vocab = {v: k for k, v in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e8c131bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_token_logits = torch.tensor(\n",
    "[4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4bc2bfcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward\n"
     ]
    }
   ],
   "source": [
    "probas = torch.softmax(next_token_logits, dim=0)\n",
    "next_token_id = torch.argmax(probas).item()\n",
    "print(inverse_vocab[next_token_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "eddef0b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "next_token_id = torch.multinomial(probas, num_samples=1).item()\n",
    "print(inverse_vocab[next_token_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7caad531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73 x closer\n",
      "0 x every\n",
      "0 x effort\n",
      "582 x forward\n",
      "2 x inches\n",
      "0 x moves\n",
      "0 x pizza\n",
      "343 x toward\n"
     ]
    }
   ],
   "source": [
    "def print_sampled_tokens(probas): \n",
    "    torch.manual_seed(123) \n",
    "    sample = [torch.multinomial(probas, num_samples=1).item() for i in range(1_000)]\n",
    "    sampled_ids = torch.bincount(torch.tensor(sample)) \n",
    "    for i , freq in enumerate(sampled_ids): \n",
    "        print(f\"{freq} x {inverse_vocab[i]}\") \n",
    "    \n",
    "\n",
    "\n",
    "print_sampled_tokens(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b0801580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temperature scaling, you divide the logits with the temp \n",
    "def softmax_with_temperature(logits, temperature):\n",
    "    scaled_logits = logits / temperature\n",
    "    return torch.softmax(scaled_logits, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d7e0168b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "temperatures = [1, 0.1, 5]\n",
    "\n",
    "scaled_probas = [softmax_with_temperature(next_token_logits,T) for T in temperatures] \n",
    "\n",
    "x = torch.arange(len(vocab)) \n",
    "bar_width  = 0.15 \n",
    "fig, ax = plt.subplots(figsize=(5,3)) \n",
    "\n",
    "for i , T in enumerate(temperatures): \n",
    "    rects = ax.bar(x + i * bar_width, scaled_probas[i],\n",
    "    bar_width,label=f\"Temperature = {T}\") \n",
    "\n",
    "ax.set_ylabel(\"probability\") \n",
    "ax.set_xticks(x) \n",
    "ax.set_xticklabels(vocab.keys(),rotation=90) \n",
    "ax.legend() \n",
    "plt.tight_layout() \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "75e1c808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top logits: tensor([6.7500, 6.2800, 4.5100])\n",
      "Top positions: tensor([3, 7, 0])\n"
     ]
    }
   ],
   "source": [
    "# Top-k sampling \n",
    "\n",
    "top_k = 3\n",
    "top_logits, top_pos = torch.topk(next_token_logits, top_k)\n",
    "print(\"Top logits:\", top_logits)\n",
    "print(\"Top positions:\", top_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "de1e11da",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_logits = torch.where(\n",
    "    condition=next_token_logits < top_logits[-1], \n",
    "    input=torch.tensor(float('-inf')), \n",
    "    other= next_token_logits\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f252d5b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.5100,   -inf,   -inf, 6.7500,   -inf,   -inf,   -inf, 6.2800,   -inf])\n"
     ]
    }
   ],
   "source": [
    "print(new_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "909d376b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0615, 0.0000, 0.0000, 0.5775, 0.0000, 0.0000, 0.0000, 0.3610, 0.0000])\n"
     ]
    }
   ],
   "source": [
    "topk_probas = torch.softmax(new_logits,dim=0) \n",
    "print(topk_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f13e468e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model,idx,max_new_tokens,context_size,temperature=0.0,top_k=None,eos_id=None):\n",
    "    for _ in range(max_new_tokens): \n",
    "        idx_cond = idx[:,-context_size:]  \n",
    "        with torch.no_grad(): \n",
    "            logits = model(idx_cond) \n",
    "        \n",
    "        logits = logits[:,-1,:]\n",
    "\n",
    "        if top_k is not None: \n",
    "            top_logits,_ = torch.topk(logits,top_k) \n",
    "            min_val = top_logits[:,-1] \n",
    "            logits = torch.where(\n",
    "                logits < min_val, \n",
    "                torch.tensor(float('-inf')).to(logits.device), \n",
    "                logits \n",
    "            )\n",
    "        if temperature > 0.0: \n",
    "            logits = logits / temperature # :) \n",
    "            probs  = torch.softmax(logits,dim=-1) \n",
    "            idx_next = torch.multinomial(probs,num_samples=1)\n",
    "        else: \n",
    "            idx_next = torch.argmax(logits,dim=-1,keepdim=True)\n",
    "        if idx_next == eos_id: \n",
    "            break \n",
    "        idx = torch.cat((idx,idx_next),dim=1) # C \n",
    "    return idx \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "270bf94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.manual_seed(123) \n",
    "\n",
    "token_ids = generate(\n",
    "    model = model, \n",
    "    idx = text_to_token(\"Every effort moves you\",tokenizer) , \n",
    "    max_new_tokens=15, \n",
    "    context_size = GPT_CONFIG_124M['context_length'], \n",
    "    top_k = 25, \n",
    "    temperature = 1.4, \n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d598239c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output text: \n",
      " Every effort moves youEveryiliaralso stabbed OrleansAllowsean 52anche crime winter unbeaten quoteembedreportprint earning\n"
     ]
    }
   ],
   "source": [
    "print(\"output text: \\n\",token_ids_to_text(token_ids,tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "578c217d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading and saving model weights in PyTorch\n",
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07dd3b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lading the model state dict \n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(torch.load(\"model.pth\", map_location=device))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe95aefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "\"model_state_dict\": model.state_dict(),\n",
    "\"optimizer_state_dict\": optimizer.state_dict(),\n",
    "},\n",
    "\"model_and_optimizer.pth\" # this is because adam has some invo for buffers to save \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6abc14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"model_and_optimizer.pth\", map_location=device) # first create a chepoint \n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.1)\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "model.train() #:) , that the workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5f263c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading pretrained weights from OpenAI \n",
    "\n",
    "\"\"\"\n",
    "Note that openai origially saved the gpt2 weights via tensorflow which we have to install \n",
    "to load the weights in python, the following code wil use a progress bar tool \n",
    "called tqdm to track the download process, which we alos have to install . \n",
    "\"\"\" \n",
    "!pip install tensorflow>=2.15.0 \n",
    "!pip install tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be58f1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request \n",
    "\n",
    "url = (\n",
    "\"https://raw.githubusercontent.com/rasbt/\"\n",
    "\"LLMs-from-scratch/main/ch05/\"\n",
    "\"01_main-chapter-code/gpt_download.py\"\n",
    ")\n",
    "filename = url.split('/')[-1]\n",
    "urllib.request.urlretrieve(url, filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec5b824",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "we can now imprt the downad_and_load_gpt2 functin from the gpt_dowload.py file \n",
    "as follows , which will load the GPT2 architecure settings and weight parameters into our python \n",
    "session: \n",
    "\"\"\" \n",
    "\n",
    "from gpt_download import download_and_load_gpt_2 #noqa \n",
    "settings, params = download_and_load_gpt2(\n",
    "model_size=\"124M\", models_dir=\"gpt2\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a9cbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Settings:\", settings)\n",
    "print(\"Parameter dictionary keys:\", params.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fff695",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_configs = {\n",
    "\"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "\"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "\"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "\"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a7090a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt2-small (124M)\"\n",
    "NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
    "NEW_CONFIG.update(model_configs[model_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270e5c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEW_CONFIG.update({\"context_length\": 1024})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27876b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEW_CONFIG.update({\"qkv_bias\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf6664d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt = GPTModel(NEW_CONFIG)\n",
    "gpt.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef5c956",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign(left, right):\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, \"\n",
    "        \"Right: {right.shape}\"\n",
    "    )\n",
    "    return torch.nn.Parameter(torch.tensor(right))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ed4a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "#... i will continue from here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c374969",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
