{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4de8b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9a431d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor(\n",
    "[[0.43, 0.15, 0.89], # Your\n",
    "[0.55, 0.87, 0.66], # journey\n",
    "[0.57, 0.85, 0.64], # starts\n",
    "[0.22, 0.58, 0.33], # with\n",
    "[0.77, 0.25, 0.10], # one\n",
    "[0.05, 0.80, 0.55]] # step\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e11d233e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "The first step of imprlementing self-attention is to compute the inntermediate values, \n",
    "(w) , refered to as attention scores, we determine these scores by computing the \n",
    "dot product of the query with every otehr input token: \n",
    "\"\"\"\n",
    "query = inputs[1] \n",
    "atten_scores_2 = torch.empty(inputs.shape[0]) \n",
    "for i , x_i in enumerate(inputs): # we enumerate over the 0-th dimention \n",
    "    atten_scores_2[i] = torch.dot(x_i, query) \n",
    "\n",
    "print(atten_scores_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6c23643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
      "Sum: tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "after calcuating attention scores we normalize them \n",
    "\"\"\" \n",
    "atten_weights_2_temp = atten_scores_2 / atten_scores_2.sum()\n",
    "print(\"Attention weights:\",atten_weights_2_temp) \n",
    "print(\"Sum:\",atten_weights_2_temp.sum()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef23d807",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "In practice , its more common and adviable to use softmax function for normalization. \n",
    "This approach is better at managing extreme valeus and offers more favorable \n",
    "gradient properteis during training. The following is a basic implementation of the \n",
    "softmax function for normalizin gthe attention scores : \n",
    "\"\"\" \n",
    "def softmax_naive(x): \n",
    "    return torch.exp(x) / torch.exp(x).sum(dim=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5107dd65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "atten_weights_2_naive = softmax_naive(atten_scores_2) \n",
    "print(\"Attention weights:\",atten_weights_2_naive) \n",
    "print(\"Sum:\",atten_weights_2_naive.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f63ab6e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "attn_weights_2 = torch.softmax(atten_scores_2, dim=0)\n",
    "print(\"Attention weights:\", attn_weights_2)\n",
    "print(\"Sum:\", attn_weights_2.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9be9c4a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "Now that we have computed the normalized attention weight , we are ready for the \n",
    "final step , caclulating the context vector z(2) by multiplying the embedded \n",
    "input token ,x(i), with the corresponding attention weights and then summing the \n",
    "resulting vectors. thus context vector z(2) is the weighte sum of all input vectors, \n",
    "obtained by multipying each input vector by its corresponding attention weight: \n",
    "\"\"\" \n",
    "\n",
    "query = inputs[1] \n",
    "context_vec_2 = torch.zeros(query.shape) \n",
    "\n",
    "for i, x_i in enumerate(inputs): \n",
    "    context_vec_2 += attn_weights_2[i]  *x_i \n",
    "\n",
    "\n",
    "print(context_vec_2 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec38bd76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = torch.empty(6, 6)\n",
    "for i, x_i in enumerate(inputs):\n",
    "    for j, x_j in enumerate(inputs):\n",
    "        attn_scores[i, j] = torch.dot(x_i, x_j)\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a17ff718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "# dont use loop use matmul like this \n",
    "attn_scores = inputs @ inputs.T\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bacdf0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
      "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
      "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
      "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
      "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
      "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n"
     ]
    }
   ],
   "source": [
    "attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6ccd57f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n"
     ]
    }
   ],
   "source": [
    "all_context_vecs = attn_weights @ inputs\n",
    "print(all_context_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fddd3eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2 = inputs[1]\n",
    "d_in = inputs.shape[1]\n",
    "d_out = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bfe5e727",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_key= torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d856e95e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4306, 1.4551])\n"
     ]
    }
   ],
   "source": [
    "query_2 = x_2 @ W_query\n",
    "key_2 = x_2 @ W_key\n",
    "value_2 = x_2 @ W_value\n",
    "print(query_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "73fe3a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys.shape: torch.Size([6, 2])\n",
      "values.shape: torch.Size([6, 2])\n"
     ]
    }
   ],
   "source": [
    "keys = inputs @ W_key\n",
    "values = inputs @ W_value\n",
    "print(\"keys.shape:\", keys.shape)\n",
    "print(\"values.shape:\", values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2d3107a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8524)\n"
     ]
    }
   ],
   "source": [
    "keys_2 = keys[1]\n",
    "attn_score_22 = query_2.dot(keys_2)\n",
    "print(attn_score_22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0bf4fa19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n"
     ]
    }
   ],
   "source": [
    "attn_scores_2 = query_2 @ keys.T\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5ff15f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' \\nThe reason for normalization by the embedding dimenstion size is to improve the training \\nperformance by avoding small gradients. For insce  when scaling up the embedding dimention, \\nwhich is typically grater than 1,000 for gpt-lie LLMs, large dot product can result \\nin very small gradients during backpropatation due to the softmax function, applied  to them . \\nas dot products incrase the softmax function behaves more like a step function \\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\" \n",
    "Now, we want to go from the atteon scores to the attention weights, we compute the \n",
    "attenion weights by scaling the attention scores and using the softmax function. \n",
    "Hower , now we scale the attention scores by dividing them by the square root of the \n",
    "embedding dimention of the key \n",
    "\"\"\" \n",
    "d_k = keys.shape[-1] \n",
    "attn_weights_2 = torch.softmax(attn_scores_2 / d_k**0.5, dim=-1)\n",
    "print(attn_weights_2)\n",
    "\"\"\" \n",
    "The reason for normalization by the embedding dimenstion size is to improve the training \n",
    "performance by avoding small gradients. For insce  when scaling up the embedding dimention, \n",
    "which is typically grater than 1,000 for gpt-lie LLMs, large dot product can result \n",
    "in very small gradients during backpropatation due to the softmax function, applied  to them . \n",
    "as dot products incrase the softmax function behaves more like a step function \n",
    "\"\"\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a1287dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3061, 0.8210])\n"
     ]
    }
   ],
   "source": [
    "context_vec_2 = attn_weights_2 @ values\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "475b1e03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6, 2]), torch.Size([6]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values.shape, attn_weights_2.shape\n",
    "# 6 @ 6 x2 -> 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "535afa79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "efb27ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention_v1(nn.Module): \n",
    "    def __init__(self,d_in,d_out): \n",
    "        super().__init__() \n",
    "        self.w_query = nn.Parameter(torch.rand(d_in,d_out)) \n",
    "        self.w_key = nn.Parameter(torch.rand(d_in,d_out)) \n",
    "        self.w_value = nn.Parameter(torch.rand(d_in,d_out))\n",
    "    \n",
    "    def forward(self,x): \n",
    "        # x (6,3)\n",
    "        #d_in = 3, d_out = 2\n",
    "        keys = x @ self.w_key #(6,3) @ (3,2 )-> 6,2 \n",
    "        queries = x @ self.w_query # 6,2\n",
    "        values  = x @ self.w_value # 6,2\n",
    "        attn_scores = queries @ keys.T # omega (6,2) @ (2,6) > 6,6 \n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1] ** 0.5 , dim=-1\n",
    "\n",
    "        ) # softmax -> sum , (6,6)\n",
    "        context_vec = attn_weights @ values #(6,6) @ (6,2) -> 6,2 \n",
    "        return context_vec \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "acac3364",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123) \n",
    "sa_v1 = SelfAttention_v1(d_in,d_out) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5596b0dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2996, 0.8053],\n",
      "        [0.3061, 0.8210],\n",
      "        [0.3058, 0.8203],\n",
      "        [0.2948, 0.7939],\n",
      "        [0.2927, 0.7891],\n",
      "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(sa_v1(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7bb09438",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 3])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6587b3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention_v2(nn.Module): \n",
    "    def __init__(self,d_in,d_out,qkv_bias=False): \n",
    "        super().__init__() \n",
    "        \"\"\"\n",
    "        self.w_query = nn.Parameter(torch.rand(d_in,d_out)) \n",
    "        self.w_key = nn.Parameter(torch.rand(d_in,d_out)) \n",
    "        self.w_value = nn.Parameter(torch.rand(d_in,d_out))\n",
    "        \"\"\" \n",
    "        self.w_query = nn.Linear(d_in,d_out,bias=qkv_bias)\n",
    "        self.w_key = nn.Linear(d_in,d_out,bias=qkv_bias)\n",
    "        self.w_value = nn.Linear(d_in,d_out,bias=qkv_bias)\n",
    "\n",
    "    \n",
    "    def forward(self,x): \n",
    "        # x (6,3)\n",
    "        #d_in = 3, d_out = 2\n",
    "        keys = self.w_key(x)\n",
    "        queries = self.w_query(x) # 6,2\n",
    "        values  = self.w_value(x) # 6,2\n",
    "        attn_scores = queries @ keys.T # omega (6,2) @ (2,6) > 6,6 \n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1] ** 0.5 , dim=-1\n",
    "\n",
    "        ) # softmax -> sum , (6,6)\n",
    "        context_vec = attn_weights @ values #(6,6) @ (6,2) -> 6,2 \n",
    "        return context_vec \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "da165bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0739,  0.0713],\n",
      "        [-0.0748,  0.0703],\n",
      "        [-0.0749,  0.0702],\n",
      "        [-0.0760,  0.0685],\n",
      "        [-0.0763,  0.0679],\n",
      "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(789)\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
    "print(sa_v2(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d3c754c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1677, 0.1666, 0.1666, 0.1662, 0.1669, 0.1660],\n",
      "        [0.1682, 0.1667, 0.1667, 0.1659, 0.1667, 0.1658],\n",
      "        [0.1682, 0.1667, 0.1667, 0.1659, 0.1667, 0.1658],\n",
      "        [0.1675, 0.1667, 0.1667, 0.1662, 0.1667, 0.1662],\n",
      "        [0.1674, 0.1667, 0.1667, 0.1663, 0.1666, 0.1663],\n",
      "        [0.1678, 0.1667, 0.1667, 0.1661, 0.1667, 0.1661]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Hiding future words with causal attention\n",
    "\n",
    "queries = sa_v2.w_query(inputs)  \n",
    "keys = sa_v2.w_key(inputs)  \n",
    "attn_scores = queries @ keys.T \n",
    "attn_weights = torch.softmax(attn_scores / keys.shape[-1]**5,dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f5828d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "context_lenght  = attn_scores.shape[0]   \n",
    "mask_simple = torch.tril(torch.ones(context_lenght,context_lenght)) #(T,T) \n",
    "print(mask_simple) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b0ad006f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1677, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1682, 0.1667, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1682, 0.1667, 0.1667, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1675, 0.1667, 0.1667, 0.1662, 0.0000, 0.0000],\n",
      "        [0.1674, 0.1667, 0.1667, 0.1663, 0.1666, 0.0000],\n",
      "        [0.1678, 0.1667, 0.1667, 0.1661, 0.1667, 0.1661]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "masked_simple = attn_weights * mask_simple \n",
    "print(masked_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e37ae85c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5023, 0.4977, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3353, 0.3323, 0.3323, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2511, 0.2498, 0.2499, 0.2492, 0.0000, 0.0000],\n",
      "        [0.2008, 0.1999, 0.1999, 0.1995, 0.1999, 0.0000],\n",
      "        [0.1678, 0.1667, 0.1667, 0.1661, 0.1667, 0.1661]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# i renomalize again \n",
    "row_sums = masked_simple.sum(dim=-1,keepdim=True) \n",
    "masked_simple_norm = masked_simple / row_sums\n",
    "print(masked_simple_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "82a383ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2899,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4656, 0.1723,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4594, 0.1703, 0.1731,   -inf,   -inf,   -inf],\n",
      "        [0.2642, 0.1024, 0.1036, 0.0186,   -inf,   -inf],\n",
      "        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786,   -inf],\n",
      "        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mask = torch.triu(torch.ones(context_lenght,context_lenght),diagonal=1) \n",
    "masked = attn_scores.masked_fill(mask.bool(),-torch.inf) \n",
    "print(masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7755bc57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.6023, 0.3977, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4289, 0.2850, 0.2861, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3031, 0.2411, 0.2416, 0.2142, 0.0000, 0.0000],\n",
      "        [0.2360, 0.1961, 0.1964, 0.1777, 0.1937, 0.0000],\n",
      "        [0.2232, 0.1649, 0.1654, 0.1417, 0.1654, 0.1393]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# now all we need to do is apply the sofmax function to these masked result and we are done \n",
    "\n",
    "attn_weights = torch.softmax(masked/ keys.shape[-1]**-0.5,dim=-1) \n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d62d223b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1855, 0.8812],\n",
      "        [0.2689, 0.9299],\n",
      "        [0.3032, 0.9453],\n",
      "        [0.2965, 0.8643],\n",
      "        [0.2689, 0.7604],\n",
      "        [0.2742, 0.7676]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "we could now use the modifed attention weights to comptue the context vectors via \n",
    "\"\"\" \n",
    "context_vec = attn_weights  @ values\n",
    "print(context_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "61d7385b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 0., 2., 2., 0.],\n",
      "        [0., 0., 0., 2., 0., 2.],\n",
      "        [2., 2., 2., 2., 0., 2.],\n",
      "        [0., 2., 2., 0., 0., 2.],\n",
      "        [0., 2., 0., 2., 0., 2.],\n",
      "        [0., 2., 2., 2., 2., 0.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123) \n",
    "dropout  = torch.nn.Dropout(0.5) \n",
    "example = torch.ones(6,6) \n",
    "print(dropout(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8f2a5dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.8579, 0.5700, 0.5722, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.4823, 0.4831, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3923, 0.0000, 0.3555, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3299, 0.3308, 0.2835, 0.3308, 0.0000]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "When you apply dropout , to comesete for the reuction in active elements, teh values of the \n",
    "remaining elements in the matrix are scaled up by a factor of 1/0.5 = 2. this \n",
    "scaling is crucial to maintain the overall baclance of the attention weights, ensuring that \n",
    "the avearage influece of the attention mechanism remains consistent during \n",
    "both the training inference phases. \n",
    "now lets apply droput to the attention weight matrix itself: \n",
    "\"\"\" \n",
    "torch.manual_seed(123) \n",
    "print(dropout(attn_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7f02bf71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "Having gained an understanding of causual attention and dropout masking , we can now develop \n",
    "a concise python class. This class id designed to facilitate the effecient application of these \n",
    "two techniques \n",
    "\"\"\" \n",
    "\n",
    "batch = torch.stack((inputs,inputs),dim=0)  \n",
    "print(batch.shape)# simulating a batch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "058a5cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CausalAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout=0.2, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "        # store mask as boolean for easier masking\n",
    "        self.register_buffer('mask', mask.bool())\n",
    "\n",
    "    def forward(self, x):  # x: (B, T, D_in)\n",
    "        B, T, D_in = x.shape\n",
    "        Q = self.W_query(x)   # (B, T, D_out)\n",
    "        K = self.W_key(x)     # (B, T, D_out)\n",
    "        V = self.W_value(x)   # (B, T, D_out)\n",
    "\n",
    "        attn_scores = Q @ K.transpose(-2, -1)  # (B, T, T)\n",
    "        attn_scores = attn_scores / (self.d_out ** 0.5)\n",
    "        attn_scores = attn_scores.masked_fill(self.mask[:T, :T], float('-inf'))\n",
    "        attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        context = attn_weights @ V  # (B, T, D_out)\n",
    "        return context\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "87f06491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch.shape: torch.Size([2, 6, 3])\n",
      "context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "# Reproducible example using the shapes you provided\n",
    "torch.manual_seed(123)\n",
    "batch = torch.randn(2, 6, 3)  # (B=2, T=6, D_in=3)\n",
    "d_in = 3\n",
    "d_out = 2\n",
    "context_length = batch.shape[1]\n",
    "\n",
    "ca = CausalAttention(d_in, d_out, context_length, dropout=0.0)\n",
    "context_vecs = ca(batch)\n",
    "\n",
    "print(\"batch.shape:\", batch.shape)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)   # expected (2,6,2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "3cae91d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiHead attetion , the long way :( \n",
    "\n",
    "class MultiHeadAttentionWrapper(nn.Module): \n",
    "    def __init__(self,d_in,d_out,\n",
    "                context_length, \n",
    "                dropout, \n",
    "                num_heads, \n",
    "                qkv_bias=False\n",
    "                ): \n",
    "        super().__init__() \n",
    "        self.heads = nn.ModuleList( \n",
    "            [\n",
    "                CausalAttention(\n",
    "                    d_in,d_out,context_lenght,dropout,qkv_bias\n",
    "                )\n",
    "                for _ in range(num_heads)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self,x): \n",
    "        return torch.cat([head(x) for head in self.heads],dim=-1) \n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "badb8219",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1] # This is the number of tokens\n",
    "d_in, d_out = 3, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "519deb6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-3.8698e-02, -7.4725e-02,  4.0897e-02,  1.8001e-02],\n",
      "         [-6.1939e-03,  4.4548e-02,  9.0551e-03, -9.7055e-04],\n",
      "         [ 2.2642e-02,  1.9516e-01,  5.8530e-04, -8.6823e-02],\n",
      "         [ 3.2217e-01, -2.5480e-01, -1.1349e-01, -8.4414e-02],\n",
      "         [-3.0068e-02, -2.2620e-01, -4.1286e-02,  2.3767e-01],\n",
      "         [ 4.4803e-02, -2.2442e-01, -1.0092e-02,  7.5202e-02]],\n",
      "\n",
      "        [[-1.5634e-01, -5.3191e-01,  7.2012e-02,  4.2994e-01],\n",
      "         [ 4.7026e-01, -2.5717e-01, -4.6930e-01, -3.4054e-02],\n",
      "         [ 7.0893e-02, -3.4483e-01, -1.2255e-01,  1.4510e-01],\n",
      "         [ 4.9815e-01, -1.4483e-01, -4.4064e-01, -5.8198e-02],\n",
      "         [ 2.7034e-01, -1.5265e-01, -3.2781e-01, -6.1827e-02],\n",
      "         [ 5.9032e-01, -1.0644e-01, -5.3756e-01, -2.0486e-01]]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 4])\n"
     ]
    }
   ],
   "source": [
    "mha = MultiHeadAttentionWrapper(\n",
    "d_in, d_out, context_length, 0.0, num_heads=2\n",
    ")\n",
    "context_vecs = mha(batch)\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "4fac9dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # ✅ fix typo and correct buffer\n",
    "        mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "        self.register_buffer(\"mask\", mask)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, D_in = x.shape\n",
    "\n",
    "        Q = self.W_query(x)  # (B, T, D_out)\n",
    "        K = self.W_key(x)    # (B, T, D_out)\n",
    "        V = self.W_value(x)  # (B, T, D_out)\n",
    "\n",
    "        # ✅ reshape for multi-heads\n",
    "        Q = Q.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)  # (B, num_heads, T, head_dim)\n",
    "        K = K.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = V.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # Attention scores\n",
    "        attn_scores = (Q @ K.transpose(-2, -1)) / (self.head_dim ** 0.5)  # (B, num_heads, T, T)\n",
    "\n",
    "        # ✅ causal mask\n",
    "        mask = self.mask[:T, :T].bool()\n",
    "        attn_scores = attn_scores.masked_fill(mask[None, None, :, :], float('-inf'))\n",
    "\n",
    "        # Softmax and dropout\n",
    "        attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Apply attention\n",
    "        context = attn_weights @ V  # (B, num_heads, T, head_dim)\n",
    "\n",
    "        # Merge heads back\n",
    "        context = context.transpose(1, 2).contiguous().view(B, T, self.d_out)\n",
    "        context = self.out_proj(context)\n",
    "\n",
    "        return context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "1bf0f8d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.1829, 0.6533],\n",
      "         [0.2044, 0.6853],\n",
      "         [0.2343, 0.7194],\n",
      "         [0.0991, 0.7718],\n",
      "         [0.1481, 0.6130],\n",
      "         [0.1328, 0.6741]],\n",
      "\n",
      "        [[0.0987, 0.5343],\n",
      "         [0.0386, 0.8978],\n",
      "         [0.1073, 0.6658],\n",
      "         [0.0658, 0.9106],\n",
      "         [0.1098, 0.8067],\n",
      "         [0.0572, 0.9746]]], grad_fn=<ViewBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "batch_size, context_length, d_in = batch.shape\n",
    "d_out = 2\n",
    "mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)\n",
    "context_vecs = mha(batch)\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa06ae6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
