{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9235f7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path \n",
    "from typing  import List "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6158838",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../the-verdict.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eeb2bb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path,'r',encoding='utf-8') as f: \n",
    "    raw_text = f.read() \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b867fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of characters:\",len(raw_text))\n",
    "print(raw_text[:99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c566e62b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello,', ' ', 'world.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test.']\n"
     ]
    }
   ],
   "source": [
    "import re # regular expresion \n",
    "\n",
    "text = \"Hello, world. This, is a test.\" \n",
    "\n",
    "result = re.split(r'(\\s)',text)\n",
    "\"\"\" \n",
    "The result is a list of individual words,whitespaces, and punctuation characters: \n",
    "\"\"\"\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16ac6e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:2: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:2: SyntaxWarning: invalid escape sequence '\\s'\n",
      "/tmp/ipykernel_30442/2659494540.py:2: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  lets mdify the regular expression splits  on whitespaces(\\s), commands and periods\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "lets mdify the regular expression splits  on whitespaces(\\s), commands and periods\n",
    "([,.]) \n",
    "\"\"\" \n",
    "result = re.split(r'([,.]|\\s)',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbf76a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2dd0879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'This', ',', 'is', 'a', 'test', '.']\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "The resulting whitespace-free output looks like as follows \n",
    "\"\"\" \n",
    "\n",
    "result = [item for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4831a9f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, world. Is this-- a test?\"\n",
    "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "result = [item.strip() for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5275b664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4690\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Now that we have a basic tokenizer working, lets apply it to Edith Whartons entire \n",
    "short story \n",
    "\"\"\" \n",
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(len(preprocessed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9ca4eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "source": [
    "print(preprocessed[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b569b352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1130\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(set(preprocessed)) \n",
    "vocab_size = len(all_words)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d507dc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {token: integer for integer, token in enumerate(all_words) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e079e3e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "(':', 8)\n",
      "(';', 9)\n",
      "('?', 10)\n",
      "('A', 11)\n",
      "('Ah', 12)\n",
      "('Among', 13)\n",
      "('And', 14)\n",
      "('Are', 15)\n",
      "('Arrt', 16)\n",
      "('As', 17)\n",
      "('At', 18)\n",
      "('Be', 19)\n",
      "('Begin', 20)\n",
      "('Burlington', 21)\n",
      "('But', 22)\n",
      "('By', 23)\n",
      "('Carlo', 24)\n",
      "('Chicago', 25)\n",
      "('Claude', 26)\n",
      "('Come', 27)\n",
      "('Croft', 28)\n",
      "('Destroyed', 29)\n",
      "('Devonshire', 30)\n",
      "('Don', 31)\n",
      "('Dubarry', 32)\n",
      "('Emperors', 33)\n",
      "('Florence', 34)\n",
      "('For', 35)\n",
      "('Gallery', 36)\n",
      "('Gideon', 37)\n",
      "('Gisburn', 38)\n",
      "('Gisburns', 39)\n",
      "('Grafton', 40)\n",
      "('Greek', 41)\n",
      "('Grindle', 42)\n",
      "('Grindles', 43)\n",
      "('HAD', 44)\n",
      "('Had', 45)\n",
      "('Hang', 46)\n",
      "('Has', 47)\n",
      "('He', 48)\n",
      "('Her', 49)\n",
      "('Hermia', 50)\n"
     ]
    }
   ],
   "source": [
    "for i , item in enumerate(vocab.items()): \n",
    "    print(item)\n",
    "    if i >=50: \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b0f75cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Lets implement a complete tokenizer class in python with an encode method that \n",
    "splits text into tokens and carreis out the string-to-integer mapping to produce \n",
    "token ids via vocabulary. In addition, well imprlement decode method that carreis \n",
    "out the reverse integer-to-string mapping to convert the token ids back into text \n",
    "\"\"\" \n",
    "\n",
    "class SimpleTokenizerV1: \n",
    "    def __init__(self,vocab:dict) ->None: # vocab -> dict \n",
    "        self.str_to_int = vocab \n",
    "        self.int_to_str = {i:s for s,i in vocab.items()} \n",
    "    \n",
    "    def encode(self,text) ->List: \n",
    "        preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text) \n",
    "        preprocessed = [\n",
    "            item.strip() for item in preprocessed if item.strip()\n",
    "        ] # remove whitespace \n",
    "        ids = [self.str_to_int[s] for s in preprocessed] \n",
    "        return ids \n",
    "\n",
    "    def decode(self,ids): \n",
    "        text = \"\".join([self.int_to_str[i] for i in ids]) \n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)# remove spaces before the \n",
    "        # specifed punctuaton \n",
    "        return text \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "204de6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0fccc633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\"It's the last he painted, you know,\"\n",
    "Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e704a77f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"It'sthelasthepainted,youknow,\"Mrs.Gisburnsaidwithpardonablepride.\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7a7b7b75",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Hello'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# try to use the tokenizer on text not in the text \u001b[39;00m\n\u001b[32m      3\u001b[39m text = \u001b[33m\"\u001b[39m\u001b[33mHello, do you like tea?\u001b[39m\u001b[33m\"\u001b[39m \n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;66;03m# we get error :(\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mSimpleTokenizerV1.encode\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m     14\u001b[39m preprocessed = re.split(\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33m([,.?_!\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m()\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[33m]|--|\u001b[39m\u001b[33m\\\u001b[39m\u001b[33ms)\u001b[39m\u001b[33m'\u001b[39m, text) \n\u001b[32m     15\u001b[39m preprocessed = [\n\u001b[32m     16\u001b[39m     item.strip() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m preprocessed \u001b[38;5;28;01mif\u001b[39;00m item.strip()\n\u001b[32m     17\u001b[39m ] \u001b[38;5;66;03m# remove whitespace \u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m ids = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstr_to_int\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m preprocessed] \n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
      "\u001b[31mKeyError\u001b[39m: 'Hello'"
     ]
    }
   ],
   "source": [
    "# try to use the tokenizer on text not in the text \n",
    "\n",
    "text = \"Hello, do you like tea?\" \n",
    "print(tokenizer.encode(text))# we get error :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dae7fd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Lets now modify the vocabulary to includde two new speical tokens <unk> and \n",
    "<|endoftext|> , by adding them to our list of all unique works: \n",
    "\"\"\" \n",
    "\n",
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "\n",
    "all_tokens.extend([\"<|endoftext|>\",\"<|unk|>\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fbcc37d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {token: integer for integer, token in enumerate(all_tokens)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4c905e59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1131"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab['<|unk|>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e095cb4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1132\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(len(vocab.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ed04c9d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('younger', 1127)\n",
      "('your', 1128)\n",
      "('yourself', 1129)\n",
      "('<|endoftext|>', 1130)\n",
      "('<|unk|>', 1131)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i , item in enumerate(list(vocab.items())[-5:]): \n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0bfd4a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV2: \n",
    "    def __init__(self,vocab:dict) ->None: # vocab -> dict \n",
    "        self.str_to_int = vocab \n",
    "        self.int_to_str = {i:s for s,i in vocab.items()} \n",
    "    \n",
    "    def encode(self,text) ->List: \n",
    "        preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text) \n",
    "        preprocessed = [\n",
    "            item.strip() for item in preprocessed if item.strip()\n",
    "        ] # remove whitespace \n",
    "        preprocessed = [item if item in self.str_to_int else f\"<|unk|>\" for item in preprocessed]\n",
    "        \n",
    "        ids = [self.str_to_int[s] for s in preprocessed] \n",
    "        return ids \n",
    "\n",
    "    def decode(self,ids): \n",
    "        text = \"\".join([self.int_to_str[i] for i in ids]) \n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)# remove spaces before the \n",
    "        # specifed punctuaton \n",
    "        return text \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "254666db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello, do you like tea?<|endoftext|>In the sunlit terraces of the place.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1 = \"hello, do you like tea?\" \n",
    "text2 = \"In the sunlit terraces of the place.\" \n",
    "\n",
    "text = \"<|endoftext|>\".join((text1,text2))\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e82742c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = SimpleTokenizerV2(vocab) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "43595f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1131, 5, 355, 1126, 628, 975, 10, 1131, 988, 956, 984, 722, 988, 773, 7]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d154b7cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|unk|>,doyouliketea?<|unk|>thesunlitterracesoftheplace.\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(tokenizer.encode(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bd53d2",
   "metadata": {},
   "source": [
    "Depending on the LLM, some researhers also consider additional speical tokens such as following: \n",
    "\n",
    "- [BOS] (begging of sequence)--This token marks the start of a text. it signifies to the llm where a piece of context begins \n",
    "- [EOS] (end of sequence)--This token is positioned at teh end of a text and is especialy useful when concatenationg multiple unrelated texts, similear to <|endoftext|>. For instace , when combining two different wikipedia articles or books, the [EOS] token indicates where once ends and the next begins . \n",
    "- [PAD] (padding)-when training LLMs with batch size larger than one, the batch might contain texts of varying lengths . To ensure all texts has the same lenght , the shorter texts are extended or `padded` using the [PAD] token , up to the lenght of the longest text in the batch. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "332a385c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c401fdab",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "26f9a57f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n"
     ]
    }
   ],
   "source": [
    "text = (\n",
    "\"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
    "\"of someunknownPlace.\"\n",
    ")\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5e80f4b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terracesof someunknownPlace.\n"
     ]
    }
   ],
   "source": [
    "strings = tokenizer.decode(integers)\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "aad8e396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5145\n"
     ]
    }
   ],
   "source": [
    "path = '../the-verdict.txt'\n",
    "\n",
    "with open(path,\"r\",encoding=\"utf-8\") as f: \n",
    "    raw_text = f.read() \n",
    "\n",
    "enc_text = tokenizer.encode(raw_text) \n",
    "\n",
    "print(len(enc_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "95fdc08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "we will remove the first 50 tokens from the dataset for demostration purposes, \n",
    "aas it results in a sligtly more interesting text passage in the next steps \n",
    "\"\"\" \n",
    "enc_sample = enc_text[50:] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "64b635d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [290, 4920, 2241, 287]\n",
      "y: [4920, 2241, 287, 257]\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "One of the easierst and most intuitive ways to create the input-target pars for the \n",
    "text-word prediction task is to crate two variables ,x and y, where x contins the input \n",
    "tokens and y contains the targets, which are the inut shifted by 1: \n",
    "\"\"\" \n",
    "\n",
    "context_size = 4 # how many tokens are included in the input \n",
    "\n",
    "x = enc_sample[:context_size] \n",
    "y= enc_sample[1:context_size+1] \n",
    "\n",
    "print(f\"x: {x}\") \n",
    "print(f\"y: {y}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ba182d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[290] -----> 4920\n",
      "[290, 4920] -----> 2241\n",
      "[290, 4920, 2241] -----> 287\n",
      "[290, 4920, 2241, 287] -----> 257\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,context_size +1): \n",
    "    context = enc_sample[:i] \n",
    "    desired = enc_sample[i] \n",
    "    print(context,\"----->\",desired) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f52cbdef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and ----->  established\n",
      " and established ----->  himself\n",
      " and established himself ----->  in\n",
      " and established himself in ----->  a\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,context_size +1): \n",
    "    context = enc_sample[:i] # upto i but not i \n",
    "    desired = enc_sample[i]\n",
    "    print(tokenizer.decode(context),\"----->\",tokenizer.decode([desired]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c37e591b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Now lets imprement dataloader for this  in pytorch \n",
    "\"\"\" \n",
    "import torch \n",
    "from torch.utils.data import Dataset, DataLoader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4b5b97c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTDatasetV1(Dataset): \n",
    "    def __init__(self,txt,tokenizer,max_length,stride): \n",
    "        self.input_ids = [] \n",
    "        self.target_ids = []\n",
    "\n",
    "        token_ids = tokenizer.encode(txt) \n",
    "\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "        \n",
    "    def __len__(self): \n",
    "        return len(self.input_ids) \n",
    "    \n",
    "    def __getitem__(self,idx): \n",
    "        return self.input_ids[idx] , self.target_ids[idx] \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d784fb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
    "                    stride=128, shuffle=True, drop_last=True,\n",
    "                    num_workers=0):\n",
    "                    \n",
    "                    tokenizer  = tiktoken.get_encoding('gpt2') \n",
    "                    dataset = GPTDatasetV1(txt,tokenizer,max_length,stride) \n",
    "                    dataloader = DataLoader(\n",
    "                        dataset, \n",
    "                        batch_size=batch_size, \n",
    "                        shuffle=shuffle, \n",
    "                        drop_last=drop_last, \n",
    "                        num_workers=num_workers\n",
    "                    )\n",
    "                    return dataloader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9b821410",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, \n",
    "    batch_size=1, \n",
    "    max_length=4, \n",
    "    stride=1, \n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1ed134b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iter = iter(dataloader) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "58e82906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n"
     ]
    }
   ],
   "source": [
    "first_batch = next(data_iter) \n",
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b1e91369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]\n"
     ]
    }
   ],
   "source": [
    "second_batch = next(data_iter) \n",
    "print(second_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5592d3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = create_dataloader_v1(\n",
    "raw_text, batch_size=8, max_length=4, stride=4,\n",
    "shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ed1f7fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Targets:\n",
      " tensor([[  367,  2885,  1464,  1807],\n",
      "        [ 3619,   402,   271, 10899],\n",
      "        [ 2138,   257,  7026, 15632],\n",
      "        [  438,  2016,   257,   922],\n",
      "        [ 5891,  1576,   438,   568],\n",
      "        [  340,   373,   645,  1049],\n",
      "        [ 5975,   284,   502,   284],\n",
      "        [ 3285,   326,    11,   287]])\n"
     ]
    }
   ],
   "source": [
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Inputs:\\n\", inputs)\n",
    "print(\"\\nTargets:\\n\", targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fa5ad9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATING TOKEN EMBEDDINGS \n",
    "\n",
    "input_ids = torch.tensor([2,3,4,1]) \n",
    "vocab_size = 5 \n",
    "output_dim = 3 \n",
    "torch.manual_seed(123) \n",
    "embedding_layer = torch.nn.Embedding(vocab_size,output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "35e39d16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 3, 4, 1])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "abd8d018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.1115,  0.1204, -0.3696],\n",
      "        [-0.2404, -1.1969,  0.2093],\n",
      "        [-0.9724, -0.7550,  0.3239],\n",
      "        [-0.1085,  0.2103, -0.3908],\n",
      "        [ 0.2350,  0.6653,  0.3528]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "621f434b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1085,  0.2103, -0.3908]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer(torch.tensor([3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7007a1df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.9724, -0.7550,  0.3239],\n",
      "        [-0.1085,  0.2103, -0.3908],\n",
      "        [ 0.2350,  0.6653,  0.3528],\n",
      "        [-0.2404, -1.1969,  0.2093]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer(input_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2b60096c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding word positons \n",
    "\"\"\"\n",
    "to achieve this , we can use two braod categoreis of position-aware embeddings: \n",
    "relative positonal embeddings and absolute positona embeddings. absolute positonal \n",
    "embeddings are directly associated with speicifc positonas in a sequence. for each \n",
    "position in the input sequence, a unique embedding is added to the token embedding to \n",
    "convery its exact location . for instance , the first token will have a specific potion \n",
    "embedding, the second token another distict embedding, and so on  , Instead of focusing \n",
    "n the absolute position of a token, the empasis of relative positonal embeddings is on \n",
    "the relative position or distance between tokens. This means the mdoel learns the \n",
    "relationship in terms of how far apart rather than at which exact position . The advantage \n",
    "here is that model can genralize better to sequences of varying lenghts , even if \n",
    "it hast seen such lenghts during training , \n",
    "opensai gpt models use absolute postional embeddings that are optimized during the trainign \n",
    "process rather than the ixed or predefined like the positonal encding in the \n",
    "original transfer model. \n",
    "\"\"\" \n",
    "vocab_size = 50257 \n",
    "output_dim  = 256 \n",
    "token_embedding_layer  = torch.nn.Embedding(vocab_size,output_dim) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c3d8d8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 4 \n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text,batch_size=8,max_length=max_length, \n",
    "    stride=max_length,shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2f8882f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Inputs shape:\n",
      " torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "data_iter = iter(dataloader) \n",
    "inputs,targets = next(data_iter) \n",
    "\n",
    "print(\"Token IDs:\\n\",inputs) \n",
    "print(\"\\nInputs shape:\\n\",inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "64a9735e",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embeddings = token_embedding_layer(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9c437941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "print(token_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7003ea69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n"
     ]
    }
   ],
   "source": [
    "\"\"\" for a gpt model absolute embeeing approach, we just need to \n",
    "crate another embedding layer that has the same embedding dimenstion as the \n",
    "token_embedding_layer \n",
    "\"\"\" \n",
    "context_lenght = max_length \n",
    "\n",
    "pos_embedding_ayer = torch.nn.Embedding(context_lenght,output_dim) \n",
    "pos_embeddings = pos_embedding_ayer(torch.arange(context_lenght)) \n",
    "print(pos_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2c43781e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "\n",
    "print(input_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86840bb7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
